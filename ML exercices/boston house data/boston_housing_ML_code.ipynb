{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRIM - per capita crime rate by town\n",
    "\n",
    "ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "INDUS - proportion of non-retail business acres per town.\n",
    "\n",
    "CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "\n",
    "NOX - nitric oxides concentration (parts per 10 million)\n",
    "\n",
    "RM - average number of rooms per dwelling\n",
    "\n",
    "AGE - proportion of owner-occupied units built prior to 1940\n",
    "\n",
    "DIS - weighted distances to five Boston employment centres\n",
    "\n",
    "RAD - index of accessibility to radial highways\n",
    "\n",
    "TAX - full-value property-tax rate per $10,000\n",
    "\n",
    "PTRATIO - pupil-teacher ratio by town\n",
    "\n",
    "B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "\n",
    "LSTAT - % lower status of the population\n",
    "\n",
    "MEDV - Median value of owner-occupied homes in $1000's\n",
    "\n",
    "https://www.youtube.com/watch?v=2vtnA9RR-VU\n",
    "https://github.com/manifoldailearning/Deep-Learning-2020/blob/master/Class_9_%26_10_Deep_learning_2020_Regression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = '''0.00632  18.00   2.310  0  0.5380  6.5750  65.20  4.0900   1  296.0  15.30\n",
    "  396.90   4.98  24.00\n",
    " 0.02731   0.00   7.070  0  0.4690  6.4210  78.90  4.9671   2  242.0  17.80\n",
    "  396.90   9.14  21.60\n",
    " 0.02729   0.00   7.070  0  0.4690  7.1850  61.10  4.9671   2  242.0  17.80\n",
    "  392.83   4.03  34.70\n",
    " 0.03237   0.00   2.180  0  0.4580  6.9980  45.80  6.0622   3  222.0  18.70\n",
    "  394.63   2.94  33.40\n",
    " 0.06905   0.00   2.180  0  0.4580  7.1470  54.20  6.0622   3  222.0  18.70\n",
    "  396.90   5.33  36.20\n",
    " 0.02985   0.00   2.180  0  0.4580  6.4300  58.70  6.0622   3  222.0  18.70\n",
    "  394.12   5.21  28.70\n",
    " 0.08829  12.50   7.870  0  0.5240  6.0120  66.60  5.5605   5  311.0  15.20\n",
    "  395.60  12.43  22.90\n",
    " 0.14455  12.50   7.870  0  0.5240  6.1720  96.10  5.9505   5  311.0  15.20\n",
    "  396.90  19.15  27.10\n",
    " 0.21124  12.50   7.870  0  0.5240  5.6310 100.00  6.0821   5  311.0  15.20\n",
    "  386.63  29.93  16.50\n",
    " 0.17004  12.50   7.870  0  0.5240  6.0040  85.90  6.5921   5  311.0  15.20\n",
    "  386.71  17.10  18.90\n",
    " 0.22489  12.50   7.870  0  0.5240  6.3770  94.30  6.3467   5  311.0  15.20\n",
    "  392.52  20.45  15.00\n",
    " 0.11747  12.50   7.870  0  0.5240  6.0090  82.90  6.2267   5  311.0  15.20\n",
    "  396.90  13.27  18.90\n",
    " 0.09378  12.50   7.870  0  0.5240  5.8890  39.00  5.4509   5  311.0  15.20\n",
    "  390.50  15.71  21.70\n",
    " 0.62976   0.00   8.140  0  0.5380  5.9490  61.80  4.7075   4  307.0  21.00\n",
    "  396.90   8.26  20.40\n",
    " 0.63796   0.00   8.140  0  0.5380  6.0960  84.50  4.4619   4  307.0  21.00\n",
    "  380.02  10.26  18.20\n",
    " 0.62739   0.00   8.140  0  0.5380  5.8340  56.50  4.4986   4  307.0  21.00\n",
    "  395.62   8.47  19.90\n",
    " 1.05393   0.00   8.140  0  0.5380  5.9350  29.30  4.4986   4  307.0  21.00\n",
    "  386.85   6.58  23.10\n",
    " 0.78420   0.00   8.140  0  0.5380  5.9900  81.70  4.2579   4  307.0  21.00\n",
    "  386.75  14.67  17.50\n",
    " 0.80271   0.00   8.140  0  0.5380  5.4560  36.60  3.7965   4  307.0  21.00\n",
    "  288.99  11.69  20.20\n",
    " 0.72580   0.00   8.140  0  0.5380  5.7270  69.50  3.7965   4  307.0  21.00\n",
    "  390.95  11.28  18.20\n",
    " 1.25179   0.00   8.140  0  0.5380  5.5700  98.10  3.7979   4  307.0  21.00\n",
    "  376.57  21.02  13.60\n",
    " 0.85204   0.00   8.140  0  0.5380  5.9650  89.20  4.0123   4  307.0  21.00\n",
    "  392.53  13.83  19.60\n",
    " 1.23247   0.00   8.140  0  0.5380  6.1420  91.70  3.9769   4  307.0  21.00\n",
    "  396.90  18.72  15.20\n",
    " 0.98843   0.00   8.140  0  0.5380  5.8130 100.00  4.0952   4  307.0  21.00\n",
    "  394.54  19.88  14.50\n",
    " 0.75026   0.00   8.140  0  0.5380  5.9240  94.10  4.3996   4  307.0  21.00\n",
    "  394.33  16.30  15.60\n",
    " 0.84054   0.00   8.140  0  0.5380  5.5990  85.70  4.4546   4  307.0  21.00\n",
    "  303.42  16.51  13.90\n",
    " 0.67191   0.00   8.140  0  0.5380  5.8130  90.30  4.6820   4  307.0  21.00\n",
    "  376.88  14.81  16.60\n",
    " 0.95577   0.00   8.140  0  0.5380  6.0470  88.80  4.4534   4  307.0  21.00\n",
    "  306.38  17.28  14.80\n",
    " 0.77299   0.00   8.140  0  0.5380  6.4950  94.40  4.4547   4  307.0  21.00\n",
    "  387.94  12.80  18.40\n",
    " 1.00245   0.00   8.140  0  0.5380  6.6740  87.30  4.2390   4  307.0  21.00\n",
    "  380.23  11.98  21.00\n",
    " 1.13081   0.00   8.140  0  0.5380  5.7130  94.10  4.2330   4  307.0  21.00\n",
    "  360.17  22.60  12.70\n",
    " 1.35472   0.00   8.140  0  0.5380  6.0720 100.00  4.1750   4  307.0  21.00\n",
    "  376.73  13.04  14.50\n",
    " 1.38799   0.00   8.140  0  0.5380  5.9500  82.00  3.9900   4  307.0  21.00\n",
    "  232.60  27.71  13.20\n",
    " 1.15172   0.00   8.140  0  0.5380  5.7010  95.00  3.7872   4  307.0  21.00\n",
    "  358.77  18.35  13.10\n",
    " 1.61282   0.00   8.140  0  0.5380  6.0960  96.90  3.7598   4  307.0  21.00\n",
    "  248.31  20.34  13.50\n",
    " 0.06417   0.00   5.960  0  0.4990  5.9330  68.20  3.3603   5  279.0  19.20\n",
    "  396.90   9.68  18.90\n",
    " 0.09744   0.00   5.960  0  0.4990  5.8410  61.40  3.3779   5  279.0  19.20\n",
    "  377.56  11.41  20.00\n",
    " 0.08014   0.00   5.960  0  0.4990  5.8500  41.50  3.9342   5  279.0  19.20\n",
    "  396.90   8.77  21.00\n",
    " 0.17505   0.00   5.960  0  0.4990  5.9660  30.20  3.8473   5  279.0  19.20\n",
    "  393.43  10.13  24.70\n",
    " 0.02763  75.00   2.950  0  0.4280  6.5950  21.80  5.4011   3  252.0  18.30\n",
    "  395.63   4.32  30.80\n",
    " 0.03359  75.00   2.950  0  0.4280  7.0240  15.80  5.4011   3  252.0  18.30\n",
    "  395.62   1.98  34.90\n",
    " 0.12744   0.00   6.910  0  0.4480  6.7700   2.90  5.7209   3  233.0  17.90\n",
    "  385.41   4.84  26.60\n",
    " 0.14150   0.00   6.910  0  0.4480  6.1690   6.60  5.7209   3  233.0  17.90\n",
    "  383.37   5.81  25.30\n",
    " 0.15936   0.00   6.910  0  0.4480  6.2110   6.50  5.7209   3  233.0  17.90\n",
    "  394.46   7.44  24.70\n",
    " 0.12269   0.00   6.910  0  0.4480  6.0690  40.00  5.7209   3  233.0  17.90\n",
    "  389.39   9.55  21.20\n",
    " 0.17142   0.00   6.910  0  0.4480  5.6820  33.80  5.1004   3  233.0  17.90\n",
    "  396.90  10.21  19.30\n",
    " 0.18836   0.00   6.910  0  0.4480  5.7860  33.30  5.1004   3  233.0  17.90\n",
    "  396.90  14.15  20.00\n",
    " 0.22927   0.00   6.910  0  0.4480  6.0300  85.50  5.6894   3  233.0  17.90\n",
    "  392.74  18.80  16.60\n",
    " 0.25387   0.00   6.910  0  0.4480  5.3990  95.30  5.8700   3  233.0  17.90\n",
    "  396.90  30.81  14.40\n",
    " 0.21977   0.00   6.910  0  0.4480  5.6020  62.00  6.0877   3  233.0  17.90\n",
    "  396.90  16.20  19.40\n",
    " 0.08873  21.00   5.640  0  0.4390  5.9630  45.70  6.8147   4  243.0  16.80\n",
    "  395.56  13.45  19.70\n",
    " 0.04337  21.00   5.640  0  0.4390  6.1150  63.00  6.8147   4  243.0  16.80\n",
    "  393.97   9.43  20.50\n",
    " 0.05360  21.00   5.640  0  0.4390  6.5110  21.10  6.8147   4  243.0  16.80\n",
    "  396.90   5.28  25.00\n",
    " 0.04981  21.00   5.640  0  0.4390  5.9980  21.40  6.8147   4  243.0  16.80\n",
    "  396.90   8.43  23.40\n",
    " 0.01360  75.00   4.000  0  0.4100  5.8880  47.60  7.3197   3  469.0  21.10\n",
    "  396.90  14.80  18.90\n",
    " 0.01311  90.00   1.220  0  0.4030  7.2490  21.90  8.6966   5  226.0  17.90\n",
    "  395.93   4.81  35.40\n",
    " 0.02055  85.00   0.740  0  0.4100  6.3830  35.70  9.1876   2  313.0  17.30\n",
    "  396.90   5.77  24.70\n",
    " 0.01432 100.00   1.320  0  0.4110  6.8160  40.50  8.3248   5  256.0  15.10\n",
    "  392.90   3.95  31.60\n",
    " 0.15445  25.00   5.130  0  0.4530  6.1450  29.20  7.8148   8  284.0  19.70\n",
    "  390.68   6.86  23.30\n",
    " 0.10328  25.00   5.130  0  0.4530  5.9270  47.20  6.9320   8  284.0  19.70\n",
    "  396.90   9.22  19.60\n",
    " 0.14932  25.00   5.130  0  0.4530  5.7410  66.20  7.2254   8  284.0  19.70\n",
    "  395.11  13.15  18.70\n",
    " 0.17171  25.00   5.130  0  0.4530  5.9660  93.40  6.8185   8  284.0  19.70\n",
    "  378.08  14.44  16.00\n",
    " 0.11027  25.00   5.130  0  0.4530  6.4560  67.80  7.2255   8  284.0  19.70\n",
    "  396.90   6.73  22.20\n",
    " 0.12650  25.00   5.130  0  0.4530  6.7620  43.40  7.9809   8  284.0  19.70\n",
    "  395.58   9.50  25.00\n",
    " 0.01951  17.50   1.380  0  0.4161  7.1040  59.50  9.2229   3  216.0  18.60\n",
    "  393.24   8.05  33.00\n",
    " 0.03584  80.00   3.370  0  0.3980  6.2900  17.80  6.6115   4  337.0  16.10\n",
    "  396.90   4.67  23.50\n",
    " 0.04379  80.00   3.370  0  0.3980  5.7870  31.10  6.6115   4  337.0  16.10\n",
    "  396.90  10.24  19.40\n",
    " 0.05789  12.50   6.070  0  0.4090  5.8780  21.40  6.4980   4  345.0  18.90\n",
    "  396.21   8.10  22.00\n",
    " 0.13554  12.50   6.070  0  0.4090  5.5940  36.80  6.4980   4  345.0  18.90\n",
    "  396.90  13.09  17.40\n",
    " 0.12816  12.50   6.070  0  0.4090  5.8850  33.00  6.4980   4  345.0  18.90\n",
    "  396.90   8.79  20.90\n",
    " 0.08826   0.00  10.810  0  0.4130  6.4170   6.60  5.2873   4  305.0  19.20\n",
    "  383.73   6.72  24.20\n",
    " 0.15876   0.00  10.810  0  0.4130  5.9610  17.50  5.2873   4  305.0  19.20\n",
    "  376.94   9.88  21.70\n",
    " 0.09164   0.00  10.810  0  0.4130  6.0650   7.80  5.2873   4  305.0  19.20\n",
    "  390.91   5.52  22.80\n",
    " 0.19539   0.00  10.810  0  0.4130  6.2450   6.20  5.2873   4  305.0  19.20\n",
    "  377.17   7.54  23.40\n",
    " 0.07896   0.00  12.830  0  0.4370  6.2730   6.00  4.2515   5  398.0  18.70\n",
    "  394.92   6.78  24.10\n",
    " 0.09512   0.00  12.830  0  0.4370  6.2860  45.00  4.5026   5  398.0  18.70\n",
    "  383.23   8.94  21.40\n",
    " 0.10153   0.00  12.830  0  0.4370  6.2790  74.50  4.0522   5  398.0  18.70\n",
    "  373.66  11.97  20.00\n",
    " 0.08707   0.00  12.830  0  0.4370  6.1400  45.80  4.0905   5  398.0  18.70\n",
    "  386.96  10.27  20.80\n",
    " 0.05646   0.00  12.830  0  0.4370  6.2320  53.70  5.0141   5  398.0  18.70\n",
    "  386.40  12.34  21.20\n",
    " 0.08387   0.00  12.830  0  0.4370  5.8740  36.60  4.5026   5  398.0  18.70\n",
    "  396.06   9.10  20.30\n",
    " 0.04113  25.00   4.860  0  0.4260  6.7270  33.50  5.4007   4  281.0  19.00\n",
    "  396.90   5.29  28.00\n",
    " 0.04462  25.00   4.860  0  0.4260  6.6190  70.40  5.4007   4  281.0  19.00\n",
    "  395.63   7.22  23.90\n",
    " 0.03659  25.00   4.860  0  0.4260  6.3020  32.20  5.4007   4  281.0  19.00\n",
    "  396.90   6.72  24.80\n",
    " 0.03551  25.00   4.860  0  0.4260  6.1670  46.70  5.4007   4  281.0  19.00\n",
    "  390.64   7.51  22.90\n",
    " 0.05059   0.00   4.490  0  0.4490  6.3890  48.00  4.7794   3  247.0  18.50\n",
    "  396.90   9.62  23.90\n",
    " 0.05735   0.00   4.490  0  0.4490  6.6300  56.10  4.4377   3  247.0  18.50\n",
    "  392.30   6.53  26.60\n",
    " 0.05188   0.00   4.490  0  0.4490  6.0150  45.10  4.4272   3  247.0  18.50\n",
    "  395.99  12.86  22.50\n",
    " 0.07151   0.00   4.490  0  0.4490  6.1210  56.80  3.7476   3  247.0  18.50\n",
    "  395.15   8.44  22.20\n",
    " 0.05660   0.00   3.410  0  0.4890  7.0070  86.30  3.4217   2  270.0  17.80\n",
    "  396.90   5.50  23.60\n",
    " 0.05302   0.00   3.410  0  0.4890  7.0790  63.10  3.4145   2  270.0  17.80\n",
    "  396.06   5.70  28.70\n",
    " 0.04684   0.00   3.410  0  0.4890  6.4170  66.10  3.0923   2  270.0  17.80\n",
    "  392.18   8.81  22.60\n",
    " 0.03932   0.00   3.410  0  0.4890  6.4050  73.90  3.0921   2  270.0  17.80\n",
    "  393.55   8.20  22.00\n",
    " 0.04203  28.00  15.040  0  0.4640  6.4420  53.60  3.6659   4  270.0  18.20\n",
    "  395.01   8.16  22.90\n",
    " 0.02875  28.00  15.040  0  0.4640  6.2110  28.90  3.6659   4  270.0  18.20\n",
    "  396.33   6.21  25.00\n",
    " 0.04294  28.00  15.040  0  0.4640  6.2490  77.30  3.6150   4  270.0  18.20\n",
    "  396.90  10.59  20.60\n",
    " 0.12204   0.00   2.890  0  0.4450  6.6250  57.80  3.4952   2  276.0  18.00\n",
    "  357.98   6.65  28.40\n",
    " 0.11504   0.00   2.890  0  0.4450  6.1630  69.60  3.4952   2  276.0  18.00\n",
    "  391.83  11.34  21.40\n",
    " 0.12083   0.00   2.890  0  0.4450  8.0690  76.00  3.4952   2  276.0  18.00\n",
    "  396.90   4.21  38.70\n",
    " 0.08187   0.00   2.890  0  0.4450  7.8200  36.90  3.4952   2  276.0  18.00\n",
    "  393.53   3.57  43.80\n",
    " 0.06860   0.00   2.890  0  0.4450  7.4160  62.50  3.4952   2  276.0  18.00\n",
    "  396.90   6.19  33.20\n",
    " 0.14866   0.00   8.560  0  0.5200  6.7270  79.90  2.7778   5  384.0  20.90\n",
    "  394.76   9.42  27.50\n",
    " 0.11432   0.00   8.560  0  0.5200  6.7810  71.30  2.8561   5  384.0  20.90\n",
    "  395.58   7.67  26.50\n",
    " 0.22876   0.00   8.560  0  0.5200  6.4050  85.40  2.7147   5  384.0  20.90\n",
    "   70.80  10.63  18.60\n",
    " 0.21161   0.00   8.560  0  0.5200  6.1370  87.40  2.7147   5  384.0  20.90\n",
    "  394.47  13.44  19.30\n",
    " 0.13960   0.00   8.560  0  0.5200  6.1670  90.00  2.4210   5  384.0  20.90\n",
    "  392.69  12.33  20.10\n",
    " 0.13262   0.00   8.560  0  0.5200  5.8510  96.70  2.1069   5  384.0  20.90\n",
    "  394.05  16.47  19.50\n",
    " 0.17120   0.00   8.560  0  0.5200  5.8360  91.90  2.2110   5  384.0  20.90\n",
    "  395.67  18.66  19.50\n",
    " 0.13117   0.00   8.560  0  0.5200  6.1270  85.20  2.1224   5  384.0  20.90\n",
    "  387.69  14.09  20.40\n",
    " 0.12802   0.00   8.560  0  0.5200  6.4740  97.10  2.4329   5  384.0  20.90\n",
    "  395.24  12.27  19.80\n",
    " 0.26363   0.00   8.560  0  0.5200  6.2290  91.20  2.5451   5  384.0  20.90\n",
    "  391.23  15.55  19.40\n",
    " 0.10793   0.00   8.560  0  0.5200  6.1950  54.40  2.7778   5  384.0  20.90\n",
    "  393.49  13.00  21.70\n",
    " 0.10084   0.00  10.010  0  0.5470  6.7150  81.60  2.6775   6  432.0  17.80\n",
    "  395.59  10.16  22.80\n",
    " 0.12329   0.00  10.010  0  0.5470  5.9130  92.90  2.3534   6  432.0  17.80\n",
    "  394.95  16.21  18.80\n",
    " 0.22212   0.00  10.010  0  0.5470  6.0920  95.40  2.5480   6  432.0  17.80\n",
    "  396.90  17.09  18.70\n",
    " 0.14231   0.00  10.010  0  0.5470  6.2540  84.20  2.2565   6  432.0  17.80\n",
    "  388.74  10.45  18.50\n",
    " 0.17134   0.00  10.010  0  0.5470  5.9280  88.20  2.4631   6  432.0  17.80\n",
    "  344.91  15.76  18.30\n",
    " 0.13158   0.00  10.010  0  0.5470  6.1760  72.50  2.7301   6  432.0  17.80\n",
    "  393.30  12.04  21.20\n",
    " 0.15098   0.00  10.010  0  0.5470  6.0210  82.60  2.7474   6  432.0  17.80\n",
    "  394.51  10.30  19.20\n",
    " 0.13058   0.00  10.010  0  0.5470  5.8720  73.10  2.4775   6  432.0  17.80\n",
    "  338.63  15.37  20.40\n",
    " 0.14476   0.00  10.010  0  0.5470  5.7310  65.20  2.7592   6  432.0  17.80\n",
    "  391.50  13.61  19.30\n",
    " 0.06899   0.00  25.650  0  0.5810  5.8700  69.70  2.2577   2  188.0  19.10\n",
    "  389.15  14.37  22.00\n",
    " 0.07165   0.00  25.650  0  0.5810  6.0040  84.10  2.1974   2  188.0  19.10\n",
    "  377.67  14.27  20.30\n",
    " 0.09299   0.00  25.650  0  0.5810  5.9610  92.90  2.0869   2  188.0  19.10\n",
    "  378.09  17.93  20.50\n",
    " 0.15038   0.00  25.650  0  0.5810  5.8560  97.00  1.9444   2  188.0  19.10\n",
    "  370.31  25.41  17.30\n",
    " 0.09849   0.00  25.650  0  0.5810  5.8790  95.80  2.0063   2  188.0  19.10\n",
    "  379.38  17.58  18.80\n",
    " 0.16902   0.00  25.650  0  0.5810  5.9860  88.40  1.9929   2  188.0  19.10\n",
    "  385.02  14.81  21.40\n",
    " 0.38735   0.00  25.650  0  0.5810  5.6130  95.60  1.7572   2  188.0  19.10\n",
    "  359.29  27.26  15.70\n",
    " 0.25915   0.00  21.890  0  0.6240  5.6930  96.00  1.7883   4  437.0  21.20\n",
    "  392.11  17.19  16.20\n",
    " 0.32543   0.00  21.890  0  0.6240  6.4310  98.80  1.8125   4  437.0  21.20\n",
    "  396.90  15.39  18.00\n",
    " 0.88125   0.00  21.890  0  0.6240  5.6370  94.70  1.9799   4  437.0  21.20\n",
    "  396.90  18.34  14.30\n",
    " 0.34006   0.00  21.890  0  0.6240  6.4580  98.90  2.1185   4  437.0  21.20\n",
    "  395.04  12.60  19.20\n",
    " 1.19294   0.00  21.890  0  0.6240  6.3260  97.70  2.2710   4  437.0  21.20\n",
    "  396.90  12.26  19.60\n",
    " 0.59005   0.00  21.890  0  0.6240  6.3720  97.90  2.3274   4  437.0  21.20\n",
    "  385.76  11.12  23.00\n",
    " 0.32982   0.00  21.890  0  0.6240  5.8220  95.40  2.4699   4  437.0  21.20\n",
    "  388.69  15.03  18.40\n",
    " 0.97617   0.00  21.890  0  0.6240  5.7570  98.40  2.3460   4  437.0  21.20\n",
    "  262.76  17.31  15.60\n",
    " 0.55778   0.00  21.890  0  0.6240  6.3350  98.20  2.1107   4  437.0  21.20\n",
    "  394.67  16.96  18.10\n",
    " 0.32264   0.00  21.890  0  0.6240  5.9420  93.50  1.9669   4  437.0  21.20\n",
    "  378.25  16.90  17.40\n",
    " 0.35233   0.00  21.890  0  0.6240  6.4540  98.40  1.8498   4  437.0  21.20\n",
    "  394.08  14.59  17.10\n",
    " 0.24980   0.00  21.890  0  0.6240  5.8570  98.20  1.6686   4  437.0  21.20\n",
    "  392.04  21.32  13.30\n",
    " 0.54452   0.00  21.890  0  0.6240  6.1510  97.90  1.6687   4  437.0  21.20\n",
    "  396.90  18.46  17.80\n",
    " 0.29090   0.00  21.890  0  0.6240  6.1740  93.60  1.6119   4  437.0  21.20\n",
    "  388.08  24.16  14.00\n",
    " 1.62864   0.00  21.890  0  0.6240  5.0190 100.00  1.4394   4  437.0  21.20\n",
    "  396.90  34.41  14.40\n",
    " 3.32105   0.00  19.580  1  0.8710  5.4030 100.00  1.3216   5  403.0  14.70\n",
    "  396.90  26.82  13.40\n",
    " 4.09740   0.00  19.580  0  0.8710  5.4680 100.00  1.4118   5  403.0  14.70\n",
    "  396.90  26.42  15.60\n",
    " 2.77974   0.00  19.580  0  0.8710  4.9030  97.80  1.3459   5  403.0  14.70\n",
    "  396.90  29.29  11.80\n",
    " 2.37934   0.00  19.580  0  0.8710  6.1300 100.00  1.4191   5  403.0  14.70\n",
    "  172.91  27.80  13.80\n",
    " 2.15505   0.00  19.580  0  0.8710  5.6280 100.00  1.5166   5  403.0  14.70\n",
    "  169.27  16.65  15.60\n",
    " 2.36862   0.00  19.580  0  0.8710  4.9260  95.70  1.4608   5  403.0  14.70\n",
    "  391.71  29.53  14.60\n",
    " 2.33099   0.00  19.580  0  0.8710  5.1860  93.80  1.5296   5  403.0  14.70\n",
    "  356.99  28.32  17.80\n",
    " 2.73397   0.00  19.580  0  0.8710  5.5970  94.90  1.5257   5  403.0  14.70\n",
    "  351.85  21.45  15.40\n",
    " 1.65660   0.00  19.580  0  0.8710  6.1220  97.30  1.6180   5  403.0  14.70\n",
    "  372.80  14.10  21.50\n",
    " 1.49632   0.00  19.580  0  0.8710  5.4040 100.00  1.5916   5  403.0  14.70\n",
    "  341.60  13.28  19.60\n",
    " 1.12658   0.00  19.580  1  0.8710  5.0120  88.00  1.6102   5  403.0  14.70\n",
    "  343.28  12.12  15.30\n",
    " 2.14918   0.00  19.580  0  0.8710  5.7090  98.50  1.6232   5  403.0  14.70\n",
    "  261.95  15.79  19.40\n",
    " 1.41385   0.00  19.580  1  0.8710  6.1290  96.00  1.7494   5  403.0  14.70\n",
    "  321.02  15.12  17.00\n",
    " 3.53501   0.00  19.580  1  0.8710  6.1520  82.60  1.7455   5  403.0  14.70\n",
    "   88.01  15.02  15.60\n",
    " 2.44668   0.00  19.580  0  0.8710  5.2720  94.00  1.7364   5  403.0  14.70\n",
    "   88.63  16.14  13.10\n",
    " 1.22358   0.00  19.580  0  0.6050  6.9430  97.40  1.8773   5  403.0  14.70\n",
    "  363.43   4.59  41.30\n",
    " 1.34284   0.00  19.580  0  0.6050  6.0660 100.00  1.7573   5  403.0  14.70\n",
    "  353.89   6.43  24.30\n",
    " 1.42502   0.00  19.580  0  0.8710  6.5100 100.00  1.7659   5  403.0  14.70\n",
    "  364.31   7.39  23.30\n",
    " 1.27346   0.00  19.580  1  0.6050  6.2500  92.60  1.7984   5  403.0  14.70\n",
    "  338.92   5.50  27.00\n",
    " 1.46336   0.00  19.580  0  0.6050  7.4890  90.80  1.9709   5  403.0  14.70\n",
    "  374.43   1.73  50.00\n",
    " 1.83377   0.00  19.580  1  0.6050  7.8020  98.20  2.0407   5  403.0  14.70\n",
    "  389.61   1.92  50.00\n",
    " 1.51902   0.00  19.580  1  0.6050  8.3750  93.90  2.1620   5  403.0  14.70\n",
    "  388.45   3.32  50.00\n",
    " 2.24236   0.00  19.580  0  0.6050  5.8540  91.80  2.4220   5  403.0  14.70\n",
    "  395.11  11.64  22.70\n",
    " 2.92400   0.00  19.580  0  0.6050  6.1010  93.00  2.2834   5  403.0  14.70\n",
    "  240.16   9.81  25.00\n",
    " 2.01019   0.00  19.580  0  0.6050  7.9290  96.20  2.0459   5  403.0  14.70\n",
    "  369.30   3.70  50.00\n",
    " 1.80028   0.00  19.580  0  0.6050  5.8770  79.20  2.4259   5  403.0  14.70\n",
    "  227.61  12.14  23.80\n",
    " 2.30040   0.00  19.580  0  0.6050  6.3190  96.10  2.1000   5  403.0  14.70\n",
    "  297.09  11.10  23.80\n",
    " 2.44953   0.00  19.580  0  0.6050  6.4020  95.20  2.2625   5  403.0  14.70\n",
    "  330.04  11.32  22.30\n",
    " 1.20742   0.00  19.580  0  0.6050  5.8750  94.60  2.4259   5  403.0  14.70\n",
    "  292.29  14.43  17.40\n",
    " 2.31390   0.00  19.580  0  0.6050  5.8800  97.30  2.3887   5  403.0  14.70\n",
    "  348.13  12.03  19.10\n",
    " 0.13914   0.00   4.050  0  0.5100  5.5720  88.50  2.5961   5  296.0  16.60\n",
    "  396.90  14.69  23.10\n",
    " 0.09178   0.00   4.050  0  0.5100  6.4160  84.10  2.6463   5  296.0  16.60\n",
    "  395.50   9.04  23.60\n",
    " 0.08447   0.00   4.050  0  0.5100  5.8590  68.70  2.7019   5  296.0  16.60\n",
    "  393.23   9.64  22.60\n",
    " 0.06664   0.00   4.050  0  0.5100  6.5460  33.10  3.1323   5  296.0  16.60\n",
    "  390.96   5.33  29.40\n",
    " 0.07022   0.00   4.050  0  0.5100  6.0200  47.20  3.5549   5  296.0  16.60\n",
    "  393.23  10.11  23.20\n",
    " 0.05425   0.00   4.050  0  0.5100  6.3150  73.40  3.3175   5  296.0  16.60\n",
    "  395.60   6.29  24.60\n",
    " 0.06642   0.00   4.050  0  0.5100  6.8600  74.40  2.9153   5  296.0  16.60\n",
    "  391.27   6.92  29.90\n",
    " 0.05780   0.00   2.460  0  0.4880  6.9800  58.40  2.8290   3  193.0  17.80\n",
    "  396.90   5.04  37.20\n",
    " 0.06588   0.00   2.460  0  0.4880  7.7650  83.30  2.7410   3  193.0  17.80\n",
    "  395.56   7.56  39.80\n",
    " 0.06888   0.00   2.460  0  0.4880  6.1440  62.20  2.5979   3  193.0  17.80\n",
    "  396.90   9.45  36.20\n",
    " 0.09103   0.00   2.460  0  0.4880  7.1550  92.20  2.7006   3  193.0  17.80\n",
    "  394.12   4.82  37.90\n",
    " 0.10008   0.00   2.460  0  0.4880  6.5630  95.60  2.8470   3  193.0  17.80\n",
    "  396.90   5.68  32.50\n",
    " 0.08308   0.00   2.460  0  0.4880  5.6040  89.80  2.9879   3  193.0  17.80\n",
    "  391.00  13.98  26.40\n",
    " 0.06047   0.00   2.460  0  0.4880  6.1530  68.80  3.2797   3  193.0  17.80\n",
    "  387.11  13.15  29.60\n",
    " 0.05602   0.00   2.460  0  0.4880  7.8310  53.60  3.1992   3  193.0  17.80\n",
    "  392.63   4.45  50.00\n",
    " 0.07875  45.00   3.440  0  0.4370  6.7820  41.10  3.7886   5  398.0  15.20\n",
    "  393.87   6.68  32.00\n",
    " 0.12579  45.00   3.440  0  0.4370  6.5560  29.10  4.5667   5  398.0  15.20\n",
    "  382.84   4.56  29.80\n",
    " 0.08370  45.00   3.440  0  0.4370  7.1850  38.90  4.5667   5  398.0  15.20\n",
    "  396.90   5.39  34.90\n",
    " 0.09068  45.00   3.440  0  0.4370  6.9510  21.50  6.4798   5  398.0  15.20\n",
    "  377.68   5.10  37.00\n",
    " 0.06911  45.00   3.440  0  0.4370  6.7390  30.80  6.4798   5  398.0  15.20\n",
    "  389.71   4.69  30.50\n",
    " 0.08664  45.00   3.440  0  0.4370  7.1780  26.30  6.4798   5  398.0  15.20\n",
    "  390.49   2.87  36.40\n",
    " 0.02187  60.00   2.930  0  0.4010  6.8000   9.90  6.2196   1  265.0  15.60\n",
    "  393.37   5.03  31.10\n",
    " 0.01439  60.00   2.930  0  0.4010  6.6040  18.80  6.2196   1  265.0  15.60\n",
    "  376.70   4.38  29.10\n",
    " 0.01381  80.00   0.460  0  0.4220  7.8750  32.00  5.6484   4  255.0  14.40\n",
    "  394.23   2.97  50.00\n",
    " 0.04011  80.00   1.520  0  0.4040  7.2870  34.10  7.3090   2  329.0  12.60\n",
    "  396.90   4.08  33.30\n",
    " 0.04666  80.00   1.520  0  0.4040  7.1070  36.60  7.3090   2  329.0  12.60\n",
    "  354.31   8.61  30.30\n",
    " 0.03768  80.00   1.520  0  0.4040  7.2740  38.30  7.3090   2  329.0  12.60\n",
    "  392.20   6.62  34.60\n",
    " 0.03150  95.00   1.470  0  0.4030  6.9750  15.30  7.6534   3  402.0  17.00\n",
    "  396.90   4.56  34.90\n",
    " 0.01778  95.00   1.470  0  0.4030  7.1350  13.90  7.6534   3  402.0  17.00\n",
    "  384.30   4.45  32.90\n",
    " 0.03445  82.50   2.030  0  0.4150  6.1620  38.40  6.2700   2  348.0  14.70\n",
    "  393.77   7.43  24.10\n",
    " 0.02177  82.50   2.030  0  0.4150  7.6100  15.70  6.2700   2  348.0  14.70\n",
    "  395.38   3.11  42.30\n",
    " 0.03510  95.00   2.680  0  0.4161  7.8530  33.20  5.1180   4  224.0  14.70\n",
    "  392.78   3.81  48.50\n",
    " 0.02009  95.00   2.680  0  0.4161  8.0340  31.90  5.1180   4  224.0  14.70\n",
    "  390.55   2.88  50.00\n",
    " 0.13642   0.00  10.590  0  0.4890  5.8910  22.30  3.9454   4  277.0  18.60\n",
    "  396.90  10.87  22.60\n",
    " 0.22969   0.00  10.590  0  0.4890  6.3260  52.50  4.3549   4  277.0  18.60\n",
    "  394.87  10.97  24.40\n",
    " 0.25199   0.00  10.590  0  0.4890  5.7830  72.70  4.3549   4  277.0  18.60\n",
    "  389.43  18.06  22.50\n",
    " 0.13587   0.00  10.590  1  0.4890  6.0640  59.10  4.2392   4  277.0  18.60\n",
    "  381.32  14.66  24.40\n",
    " 0.43571   0.00  10.590  1  0.4890  5.3440 100.00  3.8750   4  277.0  18.60\n",
    "  396.90  23.09  20.00\n",
    " 0.17446   0.00  10.590  1  0.4890  5.9600  92.10  3.8771   4  277.0  18.60\n",
    "  393.25  17.27  21.70\n",
    " 0.37578   0.00  10.590  1  0.4890  5.4040  88.60  3.6650   4  277.0  18.60\n",
    "  395.24  23.98  19.30\n",
    " 0.21719   0.00  10.590  1  0.4890  5.8070  53.80  3.6526   4  277.0  18.60\n",
    "  390.94  16.03  22.40\n",
    " 0.14052   0.00  10.590  0  0.4890  6.3750  32.30  3.9454   4  277.0  18.60\n",
    "  385.81   9.38  28.10\n",
    " 0.28955   0.00  10.590  0  0.4890  5.4120   9.80  3.5875   4  277.0  18.60\n",
    "  348.93  29.55  23.70\n",
    " 0.19802   0.00  10.590  0  0.4890  6.1820  42.40  3.9454   4  277.0  18.60\n",
    "  393.63   9.47  25.00\n",
    " 0.04560   0.00  13.890  1  0.5500  5.8880  56.00  3.1121   5  276.0  16.40\n",
    "  392.80  13.51  23.30\n",
    " 0.07013   0.00  13.890  0  0.5500  6.6420  85.10  3.4211   5  276.0  16.40\n",
    "  392.78   9.69  28.70\n",
    " 0.11069   0.00  13.890  1  0.5500  5.9510  93.80  2.8893   5  276.0  16.40\n",
    "  396.90  17.92  21.50\n",
    " 0.11425   0.00  13.890  1  0.5500  6.3730  92.40  3.3633   5  276.0  16.40\n",
    "  393.74  10.50  23.00\n",
    " 0.35809   0.00   6.200  1  0.5070  6.9510  88.50  2.8617   8  307.0  17.40\n",
    "  391.70   9.71  26.70\n",
    " 0.40771   0.00   6.200  1  0.5070  6.1640  91.30  3.0480   8  307.0  17.40\n",
    "  395.24  21.46  21.70\n",
    " 0.62356   0.00   6.200  1  0.5070  6.8790  77.70  3.2721   8  307.0  17.40\n",
    "  390.39   9.93  27.50\n",
    " 0.61470   0.00   6.200  0  0.5070  6.6180  80.80  3.2721   8  307.0  17.40\n",
    "  396.90   7.60  30.10\n",
    " 0.31533   0.00   6.200  0  0.5040  8.2660  78.30  2.8944   8  307.0  17.40\n",
    "  385.05   4.14  44.80\n",
    " 0.52693   0.00   6.200  0  0.5040  8.7250  83.00  2.8944   8  307.0  17.40\n",
    "  382.00   4.63  50.00\n",
    " 0.38214   0.00   6.200  0  0.5040  8.0400  86.50  3.2157   8  307.0  17.40\n",
    "  387.38   3.13  37.60\n",
    " 0.41238   0.00   6.200  0  0.5040  7.1630  79.90  3.2157   8  307.0  17.40\n",
    "  372.08   6.36  31.60\n",
    " 0.29819   0.00   6.200  0  0.5040  7.6860  17.00  3.3751   8  307.0  17.40\n",
    "  377.51   3.92  46.70\n",
    " 0.44178   0.00   6.200  0  0.5040  6.5520  21.40  3.3751   8  307.0  17.40\n",
    "  380.34   3.76  31.50\n",
    " 0.53700   0.00   6.200  0  0.5040  5.9810  68.10  3.6715   8  307.0  17.40\n",
    "  378.35  11.65  24.30\n",
    " 0.46296   0.00   6.200  0  0.5040  7.4120  76.90  3.6715   8  307.0  17.40\n",
    "  376.14   5.25  31.70\n",
    " 0.57529   0.00   6.200  0  0.5070  8.3370  73.30  3.8384   8  307.0  17.40\n",
    "  385.91   2.47  41.70\n",
    " 0.33147   0.00   6.200  0  0.5070  8.2470  70.40  3.6519   8  307.0  17.40\n",
    "  378.95   3.95  48.30\n",
    " 0.44791   0.00   6.200  1  0.5070  6.7260  66.50  3.6519   8  307.0  17.40\n",
    "  360.20   8.05  29.00\n",
    " 0.33045   0.00   6.200  0  0.5070  6.0860  61.50  3.6519   8  307.0  17.40\n",
    "  376.75  10.88  24.00\n",
    " 0.52058   0.00   6.200  1  0.5070  6.6310  76.50  4.1480   8  307.0  17.40\n",
    "  388.45   9.54  25.10\n",
    " 0.51183   0.00   6.200  0  0.5070  7.3580  71.60  4.1480   8  307.0  17.40\n",
    "  390.07   4.73  31.50\n",
    " 0.08244  30.00   4.930  0  0.4280  6.4810  18.50  6.1899   6  300.0  16.60\n",
    "  379.41   6.36  23.70\n",
    " 0.09252  30.00   4.930  0  0.4280  6.6060  42.20  6.1899   6  300.0  16.60\n",
    "  383.78   7.37  23.30\n",
    " 0.11329  30.00   4.930  0  0.4280  6.8970  54.30  6.3361   6  300.0  16.60\n",
    "  391.25  11.38  22.00\n",
    " 0.10612  30.00   4.930  0  0.4280  6.0950  65.10  6.3361   6  300.0  16.60\n",
    "  394.62  12.40  20.10\n",
    " 0.10290  30.00   4.930  0  0.4280  6.3580  52.90  7.0355   6  300.0  16.60\n",
    "  372.75  11.22  22.20\n",
    " 0.12757  30.00   4.930  0  0.4280  6.3930   7.80  7.0355   6  300.0  16.60\n",
    "  374.71   5.19  23.70\n",
    " 0.20608  22.00   5.860  0  0.4310  5.5930  76.50  7.9549   7  330.0  19.10\n",
    "  372.49  12.50  17.60\n",
    " 0.19133  22.00   5.860  0  0.4310  5.6050  70.20  7.9549   7  330.0  19.10\n",
    "  389.13  18.46  18.50\n",
    " 0.33983  22.00   5.860  0  0.4310  6.1080  34.90  8.0555   7  330.0  19.10\n",
    "  390.18   9.16  24.30\n",
    " 0.19657  22.00   5.860  0  0.4310  6.2260  79.20  8.0555   7  330.0  19.10\n",
    "  376.14  10.15  20.50\n",
    " 0.16439  22.00   5.860  0  0.4310  6.4330  49.10  7.8265   7  330.0  19.10\n",
    "  374.71   9.52  24.50\n",
    " 0.19073  22.00   5.860  0  0.4310  6.7180  17.50  7.8265   7  330.0  19.10\n",
    "  393.74   6.56  26.20\n",
    " 0.14030  22.00   5.860  0  0.4310  6.4870  13.00  7.3967   7  330.0  19.10\n",
    "  396.28   5.90  24.40\n",
    " 0.21409  22.00   5.860  0  0.4310  6.4380   8.90  7.3967   7  330.0  19.10\n",
    "  377.07   3.59  24.80\n",
    " 0.08221  22.00   5.860  0  0.4310  6.9570   6.80  8.9067   7  330.0  19.10\n",
    "  386.09   3.53  29.60\n",
    " 0.36894  22.00   5.860  0  0.4310  8.2590   8.40  8.9067   7  330.0  19.10\n",
    "  396.90   3.54  42.80\n",
    " 0.04819  80.00   3.640  0  0.3920  6.1080  32.00  9.2203   1  315.0  16.40\n",
    "  392.89   6.57  21.90\n",
    " 0.03548  80.00   3.640  0  0.3920  5.8760  19.10  9.2203   1  315.0  16.40\n",
    "  395.18   9.25  20.90\n",
    " 0.01538  90.00   3.750  0  0.3940  7.4540  34.20  6.3361   3  244.0  15.90\n",
    "  386.34   3.11  44.00\n",
    " 0.61154  20.00   3.970  0  0.6470  8.7040  86.90  1.8010   5  264.0  13.00\n",
    "  389.70   5.12  50.00\n",
    " 0.66351  20.00   3.970  0  0.6470  7.3330 100.00  1.8946   5  264.0  13.00\n",
    "  383.29   7.79  36.00\n",
    " 0.65665  20.00   3.970  0  0.6470  6.8420 100.00  2.0107   5  264.0  13.00\n",
    "  391.93   6.90  30.10\n",
    " 0.54011  20.00   3.970  0  0.6470  7.2030  81.80  2.1121   5  264.0  13.00\n",
    "  392.80   9.59  33.80\n",
    " 0.53412  20.00   3.970  0  0.6470  7.5200  89.40  2.1398   5  264.0  13.00\n",
    "  388.37   7.26  43.10\n",
    " 0.52014  20.00   3.970  0  0.6470  8.3980  91.50  2.2885   5  264.0  13.00\n",
    "  386.86   5.91  48.80\n",
    " 0.82526  20.00   3.970  0  0.6470  7.3270  94.50  2.0788   5  264.0  13.00\n",
    "  393.42  11.25  31.00\n",
    " 0.55007  20.00   3.970  0  0.6470  7.2060  91.60  1.9301   5  264.0  13.00\n",
    "  387.89   8.10  36.50\n",
    " 0.76162  20.00   3.970  0  0.6470  5.5600  62.80  1.9865   5  264.0  13.00\n",
    "  392.40  10.45  22.80\n",
    " 0.78570  20.00   3.970  0  0.6470  7.0140  84.60  2.1329   5  264.0  13.00\n",
    "  384.07  14.79  30.70\n",
    " 0.57834  20.00   3.970  0  0.5750  8.2970  67.00  2.4216   5  264.0  13.00\n",
    "  384.54   7.44  50.00\n",
    " 0.54050  20.00   3.970  0  0.5750  7.4700  52.60  2.8720   5  264.0  13.00\n",
    "  390.30   3.16  43.50\n",
    " 0.09065  20.00   6.960  1  0.4640  5.9200  61.50  3.9175   3  223.0  18.60\n",
    "  391.34  13.65  20.70\n",
    " 0.29916  20.00   6.960  0  0.4640  5.8560  42.10  4.4290   3  223.0  18.60\n",
    "  388.65  13.00  21.10\n",
    " 0.16211  20.00   6.960  0  0.4640  6.2400  16.30  4.4290   3  223.0  18.60\n",
    "  396.90   6.59  25.20\n",
    " 0.11460  20.00   6.960  0  0.4640  6.5380  58.70  3.9175   3  223.0  18.60\n",
    "  394.96   7.73  24.40\n",
    " 0.22188  20.00   6.960  1  0.4640  7.6910  51.80  4.3665   3  223.0  18.60\n",
    "  390.77   6.58  35.20\n",
    " 0.05644  40.00   6.410  1  0.4470  6.7580  32.90  4.0776   4  254.0  17.60\n",
    "  396.90   3.53  32.40\n",
    " 0.09604  40.00   6.410  0  0.4470  6.8540  42.80  4.2673   4  254.0  17.60\n",
    "  396.90   2.98  32.00\n",
    " 0.10469  40.00   6.410  1  0.4470  7.2670  49.00  4.7872   4  254.0  17.60\n",
    "  389.25   6.05  33.20\n",
    " 0.06127  40.00   6.410  1  0.4470  6.8260  27.60  4.8628   4  254.0  17.60\n",
    "  393.45   4.16  33.10\n",
    " 0.07978  40.00   6.410  0  0.4470  6.4820  32.10  4.1403   4  254.0  17.60\n",
    "  396.90   7.19  29.10\n",
    " 0.21038  20.00   3.330  0  0.4429  6.8120  32.20  4.1007   5  216.0  14.90\n",
    "  396.90   4.85  35.10\n",
    " 0.03578  20.00   3.330  0  0.4429  7.8200  64.50  4.6947   5  216.0  14.90\n",
    "  387.31   3.76  45.40\n",
    " 0.03705  20.00   3.330  0  0.4429  6.9680  37.20  5.2447   5  216.0  14.90\n",
    "  392.23   4.59  35.40\n",
    " 0.06129  20.00   3.330  1  0.4429  7.6450  49.70  5.2119   5  216.0  14.90\n",
    "  377.07   3.01  46.00\n",
    " 0.01501  90.00   1.210  1  0.4010  7.9230  24.80  5.8850   1  198.0  13.60\n",
    "  395.52   3.16  50.00\n",
    " 0.00906  90.00   2.970  0  0.4000  7.0880  20.80  7.3073   1  285.0  15.30\n",
    "  394.72   7.85  32.20\n",
    " 0.01096  55.00   2.250  0  0.3890  6.4530  31.90  7.3073   1  300.0  15.30\n",
    "  394.72   8.23  22.00\n",
    " 0.01965  80.00   1.760  0  0.3850  6.2300  31.50  9.0892   1  241.0  18.20\n",
    "  341.60  12.93  20.10\n",
    " 0.03871  52.50   5.320  0  0.4050  6.2090  31.30  7.3172   6  293.0  16.60\n",
    "  396.90   7.14  23.20\n",
    " 0.04590  52.50   5.320  0  0.4050  6.3150  45.60  7.3172   6  293.0  16.60\n",
    "  396.90   7.60  22.30\n",
    " 0.04297  52.50   5.320  0  0.4050  6.5650  22.90  7.3172   6  293.0  16.60\n",
    "  371.72   9.51  24.80\n",
    " 0.03502  80.00   4.950  0  0.4110  6.8610  27.90  5.1167   4  245.0  19.20\n",
    "  396.90   3.33  28.50\n",
    " 0.07886  80.00   4.950  0  0.4110  7.1480  27.70  5.1167   4  245.0  19.20\n",
    "  396.90   3.56  37.30\n",
    " 0.03615  80.00   4.950  0  0.4110  6.6300  23.40  5.1167   4  245.0  19.20\n",
    "  396.90   4.70  27.90\n",
    " 0.08265   0.00  13.920  0  0.4370  6.1270  18.40  5.5027   4  289.0  16.00\n",
    "  396.90   8.58  23.90\n",
    " 0.08199   0.00  13.920  0  0.4370  6.0090  42.30  5.5027   4  289.0  16.00\n",
    "  396.90  10.40  21.70\n",
    " 0.12932   0.00  13.920  0  0.4370  6.6780  31.10  5.9604   4  289.0  16.00\n",
    "  396.90   6.27  28.60\n",
    " 0.05372   0.00  13.920  0  0.4370  6.5490  51.00  5.9604   4  289.0  16.00\n",
    "  392.85   7.39  27.10\n",
    " 0.14103   0.00  13.920  0  0.4370  5.7900  58.00  6.3200   4  289.0  16.00\n",
    "  396.90  15.84  20.30\n",
    " 0.06466  70.00   2.240  0  0.4000  6.3450  20.10  7.8278   5  358.0  14.80\n",
    "  368.24   4.97  22.50\n",
    " 0.05561  70.00   2.240  0  0.4000  7.0410  10.00  7.8278   5  358.0  14.80\n",
    "  371.58   4.74  29.00\n",
    " 0.04417  70.00   2.240  0  0.4000  6.8710  47.40  7.8278   5  358.0  14.80\n",
    "  390.86   6.07  24.80\n",
    " 0.03537  34.00   6.090  0  0.4330  6.5900  40.40  5.4917   7  329.0  16.10\n",
    "  395.75   9.50  22.00\n",
    " 0.09266  34.00   6.090  0  0.4330  6.4950  18.40  5.4917   7  329.0  16.10\n",
    "  383.61   8.67  26.40\n",
    " 0.10000  34.00   6.090  0  0.4330  6.9820  17.70  5.4917   7  329.0  16.10\n",
    "  390.43   4.86  33.10\n",
    " 0.05515  33.00   2.180  0  0.4720  7.2360  41.10  4.0220   7  222.0  18.40\n",
    "  393.68   6.93  36.10\n",
    " 0.05479  33.00   2.180  0  0.4720  6.6160  58.10  3.3700   7  222.0  18.40\n",
    "  393.36   8.93  28.40\n",
    " 0.07503  33.00   2.180  0  0.4720  7.4200  71.90  3.0992   7  222.0  18.40\n",
    "  396.90   6.47  33.40\n",
    " 0.04932  33.00   2.180  0  0.4720  6.8490  70.30  3.1827   7  222.0  18.40\n",
    "  396.90   7.53  28.20\n",
    " 0.49298   0.00   9.900  0  0.5440  6.6350  82.50  3.3175   4  304.0  18.40\n",
    "  396.90   4.54  22.80\n",
    " 0.34940   0.00   9.900  0  0.5440  5.9720  76.70  3.1025   4  304.0  18.40\n",
    "  396.24   9.97  20.30\n",
    " 2.63548   0.00   9.900  0  0.5440  4.9730  37.80  2.5194   4  304.0  18.40\n",
    "  350.45  12.64  16.10\n",
    " 0.79041   0.00   9.900  0  0.5440  6.1220  52.80  2.6403   4  304.0  18.40\n",
    "  396.90   5.98  22.10\n",
    " 0.26169   0.00   9.900  0  0.5440  6.0230  90.40  2.8340   4  304.0  18.40\n",
    "  396.30  11.72  19.40\n",
    " 0.26938   0.00   9.900  0  0.5440  6.2660  82.80  3.2628   4  304.0  18.40\n",
    "  393.39   7.90  21.60\n",
    " 0.36920   0.00   9.900  0  0.5440  6.5670  87.30  3.6023   4  304.0  18.40\n",
    "  395.69   9.28  23.80\n",
    " 0.25356   0.00   9.900  0  0.5440  5.7050  77.70  3.9450   4  304.0  18.40\n",
    "  396.42  11.50  16.20\n",
    " 0.31827   0.00   9.900  0  0.5440  5.9140  83.20  3.9986   4  304.0  18.40\n",
    "  390.70  18.33  17.80\n",
    " 0.24522   0.00   9.900  0  0.5440  5.7820  71.70  4.0317   4  304.0  18.40\n",
    "  396.90  15.94  19.80\n",
    " 0.40202   0.00   9.900  0  0.5440  6.3820  67.20  3.5325   4  304.0  18.40\n",
    "  395.21  10.36  23.10\n",
    " 0.47547   0.00   9.900  0  0.5440  6.1130  58.80  4.0019   4  304.0  18.40\n",
    "  396.23  12.73  21.00\n",
    " 0.16760   0.00   7.380  0  0.4930  6.4260  52.30  4.5404   5  287.0  19.60\n",
    "  396.90   7.20  23.80\n",
    " 0.18159   0.00   7.380  0  0.4930  6.3760  54.30  4.5404   5  287.0  19.60\n",
    "  396.90   6.87  23.10\n",
    " 0.35114   0.00   7.380  0  0.4930  6.0410  49.90  4.7211   5  287.0  19.60\n",
    "  396.90   7.70  20.40\n",
    " 0.28392   0.00   7.380  0  0.4930  5.7080  74.30  4.7211   5  287.0  19.60\n",
    "  391.13  11.74  18.50\n",
    " 0.34109   0.00   7.380  0  0.4930  6.4150  40.10  4.7211   5  287.0  19.60\n",
    "  396.90   6.12  25.00\n",
    " 0.19186   0.00   7.380  0  0.4930  6.4310  14.70  5.4159   5  287.0  19.60\n",
    "  393.68   5.08  24.60\n",
    " 0.30347   0.00   7.380  0  0.4930  6.3120  28.90  5.4159   5  287.0  19.60\n",
    "  396.90   6.15  23.00\n",
    " 0.24103   0.00   7.380  0  0.4930  6.0830  43.70  5.4159   5  287.0  19.60\n",
    "  396.90  12.79  22.20\n",
    " 0.06617   0.00   3.240  0  0.4600  5.8680  25.80  5.2146   4  430.0  16.90\n",
    "  382.44   9.97  19.30\n",
    " 0.06724   0.00   3.240  0  0.4600  6.3330  17.20  5.2146   4  430.0  16.90\n",
    "  375.21   7.34  22.60\n",
    " 0.04544   0.00   3.240  0  0.4600  6.1440  32.20  5.8736   4  430.0  16.90\n",
    "  368.57   9.09  19.80\n",
    " 0.05023  35.00   6.060  0  0.4379  5.7060  28.40  6.6407   1  304.0  16.90\n",
    "  394.02  12.43  17.10\n",
    " 0.03466  35.00   6.060  0  0.4379  6.0310  23.30  6.6407   1  304.0  16.90\n",
    "  362.25   7.83  19.40\n",
    " 0.05083   0.00   5.190  0  0.5150  6.3160  38.10  6.4584   5  224.0  20.20\n",
    "  389.71   5.68  22.20\n",
    " 0.03738   0.00   5.190  0  0.5150  6.3100  38.50  6.4584   5  224.0  20.20\n",
    "  389.40   6.75  20.70\n",
    " 0.03961   0.00   5.190  0  0.5150  6.0370  34.50  5.9853   5  224.0  20.20\n",
    "  396.90   8.01  21.10\n",
    " 0.03427   0.00   5.190  0  0.5150  5.8690  46.30  5.2311   5  224.0  20.20\n",
    "  396.90   9.80  19.50\n",
    " 0.03041   0.00   5.190  0  0.5150  5.8950  59.60  5.6150   5  224.0  20.20\n",
    "  394.81  10.56  18.50\n",
    " 0.03306   0.00   5.190  0  0.5150  6.0590  37.30  4.8122   5  224.0  20.20\n",
    "  396.14   8.51  20.60\n",
    " 0.05497   0.00   5.190  0  0.5150  5.9850  45.40  4.8122   5  224.0  20.20\n",
    "  396.90   9.74  19.00\n",
    " 0.06151   0.00   5.190  0  0.5150  5.9680  58.50  4.8122   5  224.0  20.20\n",
    "  396.90   9.29  18.70\n",
    " 0.01301  35.00   1.520  0  0.4420  7.2410  49.30  7.0379   1  284.0  15.50\n",
    "  394.74   5.49  32.70\n",
    " 0.02498   0.00   1.890  0  0.5180  6.5400  59.70  6.2669   1  422.0  15.90\n",
    "  389.96   8.65  16.50\n",
    " 0.02543  55.00   3.780  0  0.4840  6.6960  56.40  5.7321   5  370.0  17.60\n",
    "  396.90   7.18  23.90\n",
    " 0.03049  55.00   3.780  0  0.4840  6.8740  28.10  6.4654   5  370.0  17.60\n",
    "  387.97   4.61  31.20\n",
    " 0.03113   0.00   4.390  0  0.4420  6.0140  48.50  8.0136   3  352.0  18.80\n",
    "  385.64  10.53  17.50\n",
    " 0.06162   0.00   4.390  0  0.4420  5.8980  52.30  8.0136   3  352.0  18.80\n",
    "  364.61  12.67  17.20\n",
    " 0.01870  85.00   4.150  0  0.4290  6.5160  27.70  8.5353   4  351.0  17.90\n",
    "  392.43   6.36  23.10\n",
    " 0.01501  80.00   2.010  0  0.4350  6.6350  29.70  8.3440   4  280.0  17.00\n",
    "  390.94   5.99  24.50\n",
    " 0.02899  40.00   1.250  0  0.4290  6.9390  34.50  8.7921   1  335.0  19.70\n",
    "  389.85   5.89  26.60\n",
    " 0.06211  40.00   1.250  0  0.4290  6.4900  44.40  8.7921   1  335.0  19.70\n",
    "  396.90   5.98  22.90\n",
    " 0.07950  60.00   1.690  0  0.4110  6.5790  35.90 10.7103   4  411.0  18.30\n",
    "  370.78   5.49  24.10\n",
    " 0.07244  60.00   1.690  0  0.4110  5.8840  18.50 10.7103   4  411.0  18.30\n",
    "  392.33   7.79  18.60\n",
    " 0.01709  90.00   2.020  0  0.4100  6.7280  36.10 12.1265   5  187.0  17.00\n",
    "  384.46   4.50  30.10\n",
    " 0.04301  80.00   1.910  0  0.4130  5.6630  21.90 10.5857   4  334.0  22.00\n",
    "  382.80   8.05  18.20\n",
    " 0.10659  80.00   1.910  0  0.4130  5.9360  19.50 10.5857   4  334.0  22.00\n",
    "  376.04   5.57  20.60\n",
    " 8.98296   0.00  18.100  1  0.7700  6.2120  97.40  2.1222  24  666.0  20.20\n",
    "  377.73  17.60  17.80\n",
    " 3.84970   0.00  18.100  1  0.7700  6.3950  91.00  2.5052  24  666.0  20.20\n",
    "  391.34  13.27  21.70\n",
    " 5.20177   0.00  18.100  1  0.7700  6.1270  83.40  2.7227  24  666.0  20.20\n",
    "  395.43  11.48  22.70\n",
    " 4.26131   0.00  18.100  0  0.7700  6.1120  81.30  2.5091  24  666.0  20.20\n",
    "  390.74  12.67  22.60\n",
    " 4.54192   0.00  18.100  0  0.7700  6.3980  88.00  2.5182  24  666.0  20.20\n",
    "  374.56   7.79  25.00\n",
    " 3.83684   0.00  18.100  0  0.7700  6.2510  91.10  2.2955  24  666.0  20.20\n",
    "  350.65  14.19  19.90\n",
    " 3.67822   0.00  18.100  0  0.7700  5.3620  96.20  2.1036  24  666.0  20.20\n",
    "  380.79  10.19  20.80\n",
    " 4.22239   0.00  18.100  1  0.7700  5.8030  89.00  1.9047  24  666.0  20.20\n",
    "  353.04  14.64  16.80\n",
    " 3.47428   0.00  18.100  1  0.7180  8.7800  82.90  1.9047  24  666.0  20.20\n",
    "  354.55   5.29  21.90\n",
    " 4.55587   0.00  18.100  0  0.7180  3.5610  87.90  1.6132  24  666.0  20.20\n",
    "  354.70   7.12  27.50\n",
    " 3.69695   0.00  18.100  0  0.7180  4.9630  91.40  1.7523  24  666.0  20.20\n",
    "  316.03  14.00  21.90\n",
    " 13.52220   0.00  18.100  0  0.6310  3.8630 100.00  1.5106  24  666.0  20.20\n",
    "  131.42  13.33  23.10\n",
    " 4.89822   0.00  18.100  0  0.6310  4.9700 100.00  1.3325  24  666.0  20.20\n",
    "  375.52   3.26  50.00\n",
    " 5.66998   0.00  18.100  1  0.6310  6.6830  96.80  1.3567  24  666.0  20.20\n",
    "  375.33   3.73  50.00\n",
    " 6.53876   0.00  18.100  1  0.6310  7.0160  97.50  1.2024  24  666.0  20.20\n",
    "  392.05   2.96  50.00\n",
    " 9.23230   0.00  18.100  0  0.6310  6.2160 100.00  1.1691  24  666.0  20.20\n",
    "  366.15   9.53  50.00\n",
    " 8.26725   0.00  18.100  1  0.6680  5.8750  89.60  1.1296  24  666.0  20.20\n",
    "  347.88   8.88  50.00\n",
    " 11.10810   0.00  18.100  0  0.6680  4.9060 100.00  1.1742  24  666.0  20.20\n",
    "  396.90  34.77  13.80\n",
    " 18.49820   0.00  18.100  0  0.6680  4.1380 100.00  1.1370  24  666.0  20.20\n",
    "  396.90  37.97  13.80\n",
    " 19.60910   0.00  18.100  0  0.6710  7.3130  97.90  1.3163  24  666.0  20.20\n",
    "  396.90  13.44  15.00\n",
    " 15.28800   0.00  18.100  0  0.6710  6.6490  93.30  1.3449  24  666.0  20.20\n",
    "  363.02  23.24  13.90\n",
    " 9.82349   0.00  18.100  0  0.6710  6.7940  98.80  1.3580  24  666.0  20.20\n",
    "  396.90  21.24  13.30\n",
    " 23.64820   0.00  18.100  0  0.6710  6.3800  96.20  1.3861  24  666.0  20.20\n",
    "  396.90  23.69  13.10\n",
    " 17.86670   0.00  18.100  0  0.6710  6.2230 100.00  1.3861  24  666.0  20.20\n",
    "  393.74  21.78  10.20\n",
    " 88.97620   0.00  18.100  0  0.6710  6.9680  91.90  1.4165  24  666.0  20.20\n",
    "  396.90  17.21  10.40\n",
    " 15.87440   0.00  18.100  0  0.6710  6.5450  99.10  1.5192  24  666.0  20.20\n",
    "  396.90  21.08  10.90\n",
    " 9.18702   0.00  18.100  0  0.7000  5.5360 100.00  1.5804  24  666.0  20.20\n",
    "  396.90  23.60  11.30\n",
    " 7.99248   0.00  18.100  0  0.7000  5.5200 100.00  1.5331  24  666.0  20.20\n",
    "  396.90  24.56  12.30\n",
    " 20.08490   0.00  18.100  0  0.7000  4.3680  91.20  1.4395  24  666.0  20.20\n",
    "  285.83  30.63   8.80\n",
    " 16.81180   0.00  18.100  0  0.7000  5.2770  98.10  1.4261  24  666.0  20.20\n",
    "  396.90  30.81   7.20\n",
    " 24.39380   0.00  18.100  0  0.7000  4.6520 100.00  1.4672  24  666.0  20.20\n",
    "  396.90  28.28  10.50\n",
    " 22.59710   0.00  18.100  0  0.7000  5.0000  89.50  1.5184  24  666.0  20.20\n",
    "  396.90  31.99   7.40\n",
    " 14.33370   0.00  18.100  0  0.7000  4.8800 100.00  1.5895  24  666.0  20.20\n",
    "  372.92  30.62  10.20\n",
    " 8.15174   0.00  18.100  0  0.7000  5.3900  98.90  1.7281  24  666.0  20.20\n",
    "  396.90  20.85  11.50\n",
    " 6.96215   0.00  18.100  0  0.7000  5.7130  97.00  1.9265  24  666.0  20.20\n",
    "  394.43  17.11  15.10\n",
    " 5.29305   0.00  18.100  0  0.7000  6.0510  82.50  2.1678  24  666.0  20.20\n",
    "  378.38  18.76  23.20\n",
    " 11.57790   0.00  18.100  0  0.7000  5.0360  97.00  1.7700  24  666.0  20.20\n",
    "  396.90  25.68   9.70\n",
    " 8.64476   0.00  18.100  0  0.6930  6.1930  92.60  1.7912  24  666.0  20.20\n",
    "  396.90  15.17  13.80\n",
    " 13.35980   0.00  18.100  0  0.6930  5.8870  94.70  1.7821  24  666.0  20.20\n",
    "  396.90  16.35  12.70\n",
    " 8.71675   0.00  18.100  0  0.6930  6.4710  98.80  1.7257  24  666.0  20.20\n",
    "  391.98  17.12  13.10\n",
    " 5.87205   0.00  18.100  0  0.6930  6.4050  96.00  1.6768  24  666.0  20.20\n",
    "  396.90  19.37  12.50\n",
    " 7.67202   0.00  18.100  0  0.6930  5.7470  98.90  1.6334  24  666.0  20.20\n",
    "  393.10  19.92   8.50\n",
    " 38.35180   0.00  18.100  0  0.6930  5.4530 100.00  1.4896  24  666.0  20.20\n",
    "  396.90  30.59   5.00\n",
    " 9.91655   0.00  18.100  0  0.6930  5.8520  77.80  1.5004  24  666.0  20.20\n",
    "  338.16  29.97   6.30\n",
    " 25.04610   0.00  18.100  0  0.6930  5.9870 100.00  1.5888  24  666.0  20.20\n",
    "  396.90  26.77   5.60\n",
    " 14.23620   0.00  18.100  0  0.6930  6.3430 100.00  1.5741  24  666.0  20.20\n",
    "  396.90  20.32   7.20\n",
    " 9.59571   0.00  18.100  0  0.6930  6.4040 100.00  1.6390  24  666.0  20.20\n",
    "  376.11  20.31  12.10\n",
    " 24.80170   0.00  18.100  0  0.6930  5.3490  96.00  1.7028  24  666.0  20.20\n",
    "  396.90  19.77   8.30\n",
    " 41.52920   0.00  18.100  0  0.6930  5.5310  85.40  1.6074  24  666.0  20.20\n",
    "  329.46  27.38   8.50\n",
    " 67.92080   0.00  18.100  0  0.6930  5.6830 100.00  1.4254  24  666.0  20.20\n",
    "  384.97  22.98   5.00\n",
    " 20.71620   0.00  18.100  0  0.6590  4.1380 100.00  1.1781  24  666.0  20.20\n",
    "  370.22  23.34  11.90\n",
    " 11.95110   0.00  18.100  0  0.6590  5.6080 100.00  1.2852  24  666.0  20.20\n",
    "  332.09  12.13  27.90\n",
    " 7.40389   0.00  18.100  0  0.5970  5.6170  97.90  1.4547  24  666.0  20.20\n",
    "  314.64  26.40  17.20\n",
    " 14.43830   0.00  18.100  0  0.5970  6.8520 100.00  1.4655  24  666.0  20.20\n",
    "  179.36  19.78  27.50\n",
    " 51.13580   0.00  18.100  0  0.5970  5.7570 100.00  1.4130  24  666.0  20.20\n",
    "    2.60  10.11  15.00\n",
    " 14.05070   0.00  18.100  0  0.5970  6.6570 100.00  1.5275  24  666.0  20.20\n",
    "   35.05  21.22  17.20\n",
    " 18.81100   0.00  18.100  0  0.5970  4.6280 100.00  1.5539  24  666.0  20.20\n",
    "   28.79  34.37  17.90\n",
    " 28.65580   0.00  18.100  0  0.5970  5.1550 100.00  1.5894  24  666.0  20.20\n",
    "  210.97  20.08  16.30\n",
    " 45.74610   0.00  18.100  0  0.6930  4.5190 100.00  1.6582  24  666.0  20.20\n",
    "   88.27  36.98   7.00\n",
    " 18.08460   0.00  18.100  0  0.6790  6.4340 100.00  1.8347  24  666.0  20.20\n",
    "   27.25  29.05   7.20\n",
    " 10.83420   0.00  18.100  0  0.6790  6.7820  90.80  1.8195  24  666.0  20.20\n",
    "   21.57  25.79   7.50\n",
    " 25.94060   0.00  18.100  0  0.6790  5.3040  89.10  1.6475  24  666.0  20.20\n",
    "  127.36  26.64  10.40\n",
    " 73.53410   0.00  18.100  0  0.6790  5.9570 100.00  1.8026  24  666.0  20.20\n",
    "   16.45  20.62   8.80\n",
    " 11.81230   0.00  18.100  0  0.7180  6.8240  76.50  1.7940  24  666.0  20.20\n",
    "   48.45  22.74   8.40\n",
    " 11.08740   0.00  18.100  0  0.7180  6.4110 100.00  1.8589  24  666.0  20.20\n",
    "  318.75  15.02  16.70\n",
    " 7.02259   0.00  18.100  0  0.7180  6.0060  95.30  1.8746  24  666.0  20.20\n",
    "  319.98  15.70  14.20\n",
    " 12.04820   0.00  18.100  0  0.6140  5.6480  87.60  1.9512  24  666.0  20.20\n",
    "  291.55  14.10  20.80\n",
    " 7.05042   0.00  18.100  0  0.6140  6.1030  85.10  2.0218  24  666.0  20.20\n",
    "    2.52  23.29  13.40\n",
    " 8.79212   0.00  18.100  0  0.5840  5.5650  70.60  2.0635  24  666.0  20.20\n",
    "    3.65  17.16  11.70\n",
    " 15.86030   0.00  18.100  0  0.6790  5.8960  95.40  1.9096  24  666.0  20.20\n",
    "    7.68  24.39   8.30\n",
    " 12.24720   0.00  18.100  0  0.5840  5.8370  59.70  1.9976  24  666.0  20.20\n",
    "   24.65  15.69  10.20\n",
    " 37.66190   0.00  18.100  0  0.6790  6.2020  78.70  1.8629  24  666.0  20.20\n",
    "   18.82  14.52  10.90\n",
    " 7.36711   0.00  18.100  0  0.6790  6.1930  78.10  1.9356  24  666.0  20.20\n",
    "   96.73  21.52  11.00\n",
    " 9.33889   0.00  18.100  0  0.6790  6.3800  95.60  1.9682  24  666.0  20.20\n",
    "   60.72  24.08   9.50\n",
    " 8.49213   0.00  18.100  0  0.5840  6.3480  86.10  2.0527  24  666.0  20.20\n",
    "   83.45  17.64  14.50\n",
    " 10.06230   0.00  18.100  0  0.5840  6.8330  94.30  2.0882  24  666.0  20.20\n",
    "   81.33  19.69  14.10\n",
    " 6.44405   0.00  18.100  0  0.5840  6.4250  74.80  2.2004  24  666.0  20.20\n",
    "   97.95  12.03  16.10\n",
    " 5.58107   0.00  18.100  0  0.7130  6.4360  87.90  2.3158  24  666.0  20.20\n",
    "  100.19  16.22  14.30\n",
    " 13.91340   0.00  18.100  0  0.7130  6.2080  95.00  2.2222  24  666.0  20.20\n",
    "  100.63  15.17  11.70\n",
    " 11.16040   0.00  18.100  0  0.7400  6.6290  94.60  2.1247  24  666.0  20.20\n",
    "  109.85  23.27  13.40\n",
    " 14.42080   0.00  18.100  0  0.7400  6.4610  93.30  2.0026  24  666.0  20.20\n",
    "   27.49  18.05   9.60\n",
    " 15.17720   0.00  18.100  0  0.7400  6.1520 100.00  1.9142  24  666.0  20.20\n",
    "    9.32  26.45   8.70\n",
    " 13.67810   0.00  18.100  0  0.7400  5.9350  87.90  1.8206  24  666.0  20.20\n",
    "   68.95  34.02   8.40\n",
    " 9.39063   0.00  18.100  0  0.7400  5.6270  93.90  1.8172  24  666.0  20.20\n",
    "  396.90  22.88  12.80\n",
    " 22.05110   0.00  18.100  0  0.7400  5.8180  92.40  1.8662  24  666.0  20.20\n",
    "  391.45  22.11  10.50\n",
    " 9.72418   0.00  18.100  0  0.7400  6.4060  97.20  2.0651  24  666.0  20.20\n",
    "  385.96  19.52  17.10\n",
    " 5.66637   0.00  18.100  0  0.7400  6.2190 100.00  2.0048  24  666.0  20.20\n",
    "  395.69  16.59  18.40\n",
    " 9.96654   0.00  18.100  0  0.7400  6.4850 100.00  1.9784  24  666.0  20.20\n",
    "  386.73  18.85  15.40\n",
    " 12.80230   0.00  18.100  0  0.7400  5.8540  96.60  1.8956  24  666.0  20.20\n",
    "  240.52  23.79  10.80\n",
    " 10.67180   0.00  18.100  0  0.7400  6.4590  94.80  1.9879  24  666.0  20.20\n",
    "   43.06  23.98  11.80\n",
    " 6.28807   0.00  18.100  0  0.7400  6.3410  96.40  2.0720  24  666.0  20.20\n",
    "  318.01  17.79  14.90\n",
    " 9.92485   0.00  18.100  0  0.7400  6.2510  96.60  2.1980  24  666.0  20.20\n",
    "  388.52  16.44  12.60\n",
    " 9.32909   0.00  18.100  0  0.7130  6.1850  98.70  2.2616  24  666.0  20.20\n",
    "  396.90  18.13  14.10\n",
    " 7.52601   0.00  18.100  0  0.7130  6.4170  98.30  2.1850  24  666.0  20.20\n",
    "  304.21  19.31  13.00\n",
    " 6.71772   0.00  18.100  0  0.7130  6.7490  92.60  2.3236  24  666.0  20.20\n",
    "    0.32  17.44  13.40\n",
    " 5.44114   0.00  18.100  0  0.7130  6.6550  98.20  2.3552  24  666.0  20.20\n",
    "  355.29  17.73  15.20\n",
    " 5.09017   0.00  18.100  0  0.7130  6.2970  91.80  2.3682  24  666.0  20.20\n",
    "  385.09  17.27  16.10\n",
    " 8.24809   0.00  18.100  0  0.7130  7.3930  99.30  2.4527  24  666.0  20.20\n",
    "  375.87  16.74  17.80\n",
    " 9.51363   0.00  18.100  0  0.7130  6.7280  94.10  2.4961  24  666.0  20.20\n",
    "    6.68  18.71  14.90\n",
    " 4.75237   0.00  18.100  0  0.7130  6.5250  86.50  2.4358  24  666.0  20.20\n",
    "   50.92  18.13  14.10\n",
    " 4.66883   0.00  18.100  0  0.7130  5.9760  87.90  2.5806  24  666.0  20.20\n",
    "   10.48  19.01  12.70\n",
    " 8.20058   0.00  18.100  0  0.7130  5.9360  80.30  2.7792  24  666.0  20.20\n",
    "    3.50  16.94  13.50\n",
    " 7.75223   0.00  18.100  0  0.7130  6.3010  83.70  2.7831  24  666.0  20.20\n",
    "  272.21  16.23  14.90\n",
    " 6.80117   0.00  18.100  0  0.7130  6.0810  84.40  2.7175  24  666.0  20.20\n",
    "  396.90  14.70  20.00\n",
    " 4.81213   0.00  18.100  0  0.7130  6.7010  90.00  2.5975  24  666.0  20.20\n",
    "  255.23  16.42  16.40\n",
    " 3.69311   0.00  18.100  0  0.7130  6.3760  88.40  2.5671  24  666.0  20.20\n",
    "  391.43  14.65  17.70\n",
    " 6.65492   0.00  18.100  0  0.7130  6.3170  83.00  2.7344  24  666.0  20.20\n",
    "  396.90  13.99  19.50\n",
    " 5.82115   0.00  18.100  0  0.7130  6.5130  89.90  2.8016  24  666.0  20.20\n",
    "  393.82  10.29  20.20\n",
    " 7.83932   0.00  18.100  0  0.6550  6.2090  65.40  2.9634  24  666.0  20.20\n",
    "  396.90  13.22  21.40\n",
    " 3.16360   0.00  18.100  0  0.6550  5.7590  48.20  3.0665  24  666.0  20.20\n",
    "  334.40  14.13  19.90\n",
    " 3.77498   0.00  18.100  0  0.6550  5.9520  84.70  2.8715  24  666.0  20.20\n",
    "   22.01  17.15  19.00\n",
    " 4.42228   0.00  18.100  0  0.5840  6.0030  94.50  2.5403  24  666.0  20.20\n",
    "  331.29  21.32  19.10\n",
    " 15.57570   0.00  18.100  0  0.5800  5.9260  71.00  2.9084  24  666.0  20.20\n",
    "  368.74  18.13  19.10\n",
    " 13.07510   0.00  18.100  0  0.5800  5.7130  56.70  2.8237  24  666.0  20.20\n",
    "  396.90  14.76  20.10\n",
    " 4.34879   0.00  18.100  0  0.5800  6.1670  84.00  3.0334  24  666.0  20.20\n",
    "  396.90  16.29  19.90\n",
    " 4.03841   0.00  18.100  0  0.5320  6.2290  90.70  3.0993  24  666.0  20.20\n",
    "  395.33  12.87  19.60\n",
    " 3.56868   0.00  18.100  0  0.5800  6.4370  75.00  2.8965  24  666.0  20.20\n",
    "  393.37  14.36  23.20\n",
    " 4.64689   0.00  18.100  0  0.6140  6.9800  67.60  2.5329  24  666.0  20.20\n",
    "  374.68  11.66  29.80\n",
    " 8.05579   0.00  18.100  0  0.5840  5.4270  95.40  2.4298  24  666.0  20.20\n",
    "  352.58  18.14  13.80\n",
    " 6.39312   0.00  18.100  0  0.5840  6.1620  97.40  2.2060  24  666.0  20.20\n",
    "  302.76  24.10  13.30\n",
    " 4.87141   0.00  18.100  0  0.6140  6.4840  93.60  2.3053  24  666.0  20.20\n",
    "  396.21  18.68  16.70\n",
    " 15.02340   0.00  18.100  0  0.6140  5.3040  97.30  2.1007  24  666.0  20.20\n",
    "  349.48  24.91  12.00\n",
    " 10.23300   0.00  18.100  0  0.6140  6.1850  96.70  2.1705  24  666.0  20.20\n",
    "  379.70  18.03  14.60\n",
    " 14.33370   0.00  18.100  0  0.6140  6.2290  88.00  1.9512  24  666.0  20.20\n",
    "  383.32  13.11  21.40\n",
    " 5.82401   0.00  18.100  0  0.5320  6.2420  64.70  3.4242  24  666.0  20.20\n",
    "  396.90  10.74  23.00\n",
    " 5.70818   0.00  18.100  0  0.5320  6.7500  74.90  3.3317  24  666.0  20.20\n",
    "  393.07   7.74  23.70\n",
    " 5.73116   0.00  18.100  0  0.5320  7.0610  77.00  3.4106  24  666.0  20.20\n",
    "  395.28   7.01  25.00\n",
    " 2.81838   0.00  18.100  0  0.5320  5.7620  40.30  4.0983  24  666.0  20.20\n",
    "  392.92  10.42  21.80\n",
    " 2.37857   0.00  18.100  0  0.5830  5.8710  41.90  3.7240  24  666.0  20.20\n",
    "  370.73  13.34  20.60\n",
    " 3.67367   0.00  18.100  0  0.5830  6.3120  51.90  3.9917  24  666.0  20.20\n",
    "  388.62  10.58  21.20\n",
    " 5.69175   0.00  18.100  0  0.5830  6.1140  79.80  3.5459  24  666.0  20.20\n",
    "  392.68  14.98  19.10\n",
    " 4.83567   0.00  18.100  0  0.5830  5.9050  53.20  3.1523  24  666.0  20.20\n",
    "  388.22  11.45  20.60\n",
    " 0.15086   0.00  27.740  0  0.6090  5.4540  92.70  1.8209   4  711.0  20.10\n",
    "  395.09  18.06  15.20\n",
    " 0.18337   0.00  27.740  0  0.6090  5.4140  98.30  1.7554   4  711.0  20.10\n",
    "  344.05  23.97   7.00\n",
    " 0.20746   0.00  27.740  0  0.6090  5.0930  98.00  1.8226   4  711.0  20.10\n",
    "  318.43  29.68   8.10\n",
    " 0.10574   0.00  27.740  0  0.6090  5.9830  98.80  1.8681   4  711.0  20.10\n",
    "  390.11  18.07  13.60\n",
    " 0.11132   0.00  27.740  0  0.6090  5.9830  83.50  2.1099   4  711.0  20.10\n",
    "  396.90  13.35  20.10\n",
    " 0.17331   0.00   9.690  0  0.5850  5.7070  54.00  2.3817   6  391.0  19.20\n",
    "  396.90  12.01  21.80\n",
    " 0.27957   0.00   9.690  0  0.5850  5.9260  42.60  2.3817   6  391.0  19.20\n",
    "  396.90  13.59  24.50\n",
    " 0.17899   0.00   9.690  0  0.5850  5.6700  28.80  2.7986   6  391.0  19.20\n",
    "  393.29  17.60  23.10\n",
    " 0.28960   0.00   9.690  0  0.5850  5.3900  72.90  2.7986   6  391.0  19.20\n",
    "  396.90  21.14  19.70\n",
    " 0.26838   0.00   9.690  0  0.5850  5.7940  70.60  2.8927   6  391.0  19.20\n",
    "  396.90  14.10  18.30\n",
    " 0.23912   0.00   9.690  0  0.5850  6.0190  65.30  2.4091   6  391.0  19.20\n",
    "  396.90  12.92  21.20\n",
    " 0.17783   0.00   9.690  0  0.5850  5.5690  73.50  2.3999   6  391.0  19.20\n",
    "  395.77  15.10  17.50\n",
    " 0.22438   0.00   9.690  0  0.5850  6.0270  79.70  2.4982   6  391.0  19.20\n",
    "  396.90  14.33  16.80\n",
    " 0.06263   0.00  11.930  0  0.5730  6.5930  69.10  2.4786   1  273.0  21.00\n",
    "  391.99   9.67  22.40\n",
    " 0.04527   0.00  11.930  0  0.5730  6.1200  76.70  2.2875   1  273.0  21.00\n",
    "  396.90   9.08  20.60\n",
    " 0.06076   0.00  11.930  0  0.5730  6.9760  91.00  2.1675   1  273.0  21.00\n",
    "  396.90   5.64  23.90\n",
    " 0.10959   0.00  11.930  0  0.5730  6.7940  89.30  2.3889   1  273.0  21.00\n",
    "  393.45   6.48  22.00\n",
    " 0.04741   0.00  11.930  0  0.5730  6.0300  80.80  2.5050   1  273.0  21.00\n",
    "  396.90   7.88  11.90'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n=0\n",
    "m=0\n",
    "for i in dados:\n",
    "\n",
    "    if i != \" \":\n",
    "        n=1\n",
    "\n",
    "    if i == \" \" and n==1:\n",
    "        n=0\n",
    "        m+=1\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados  = dados.replace(\"\\n\", \"\")\n",
    "dados  = dados.replace(\"   \", \" \")\n",
    "dados  = dados.replace(\"  \", \" \")\n",
    "dados = dados.split(\" \")\n",
    "dados = list(map(float, dados))\n",
    "\n",
    "\n",
    "# Transformar em uma lista de listas de 14 elementos\n",
    "lista_de_listas = [dados[i:i+14] for i in range(0, len(dados), 14)]\n",
    "\n",
    "# Converter a lista em um DataFrame\n",
    "df = pd.DataFrame(lista_de_listas, columns=[\n",
    "    \"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\",\n",
    "    \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('C:/Users/felip/Documents/UFPB/Estudos/ML Google/ML exerc youtube 2/boston_house_fv.txt', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = df.mean()\n",
    "df_std = df.std()\n",
    "df_normalized = (df - df_mean) / df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df_normalized.drop([\"MEDV\"], axis=1)\n",
    "y = df_normalized.MEDV\n",
    "train_data, test_data, train_label, test_label = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "test_data = {name:np.array(value) for name, value in test_data.items()} #transformando em dicionario\n",
    "test_label = test_label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>506.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM    ZN  INDUS  CHAS   NOX    RM   AGE   DIS   RAD   TAX  PTRATIO  \\\n",
       "count 506.0 506.0  506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0    506.0   \n",
       "mean   -0.0   0.0    0.0  -0.0  -0.0  -0.0  -0.0  -0.0   0.0   0.0     -0.0   \n",
       "std     1.0   1.0    1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0      1.0   \n",
       "min    -0.4  -0.5   -1.6  -0.3  -1.5  -3.9  -2.3  -1.3  -1.0  -1.3     -2.7   \n",
       "25%    -0.4  -0.5   -0.9  -0.3  -0.9  -0.6  -0.8  -0.8  -0.6  -0.8     -0.5   \n",
       "50%    -0.4  -0.5   -0.2  -0.3  -0.1  -0.1   0.3  -0.3  -0.5  -0.5      0.3   \n",
       "75%     0.0   0.0    1.0  -0.3   0.6   0.5   0.9   0.7   1.7   1.5      0.8   \n",
       "max     9.9   3.8    2.4   3.7   2.7   3.6   1.1   4.0   1.7   1.8      1.6   \n",
       "\n",
       "          B  LSTAT  MEDV  \n",
       "count 506.0  506.0 506.0  \n",
       "mean   -0.0   -0.0  -0.0  \n",
       "std     1.0    1.0   1.0  \n",
       "min    -3.9   -1.5  -1.9  \n",
       "25%     0.2   -0.8  -0.6  \n",
       "50%     0.4   -0.2  -0.1  \n",
       "75%     0.4    0.6   0.3  \n",
       "max     0.4    3.5   3.0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normalized.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "#ss = StandardScaler()\n",
    "#transformed_train_data = ss.fit_transform(train_data)\n",
    "#transformed_test_data = ss.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature layer\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column(col) for col in train_data.columns if col != 'MEDV']\n",
    "#não usar mais tf.feature_column pois será removido do tensor flow, o Keras deverá ser usado no lugar\n",
    "\n",
    "my_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_the_loss_curve function.\n"
     ]
    }
   ],
   "source": [
    "def plot_the_loss_curve(epochs, mse):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "\n",
    "    plt.plot(epochs, mse, label = \"LOSS\")\n",
    "    plt.legend()\n",
    "    plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
    "    plt.show()\n",
    "\n",
    "print(\"Defined the plot_the_loss_curve function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define functions to create and train a linear regression model\n",
    "def create_model(my_learning_rate, feature_layer):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Add the layer containing the feature columns to the model.\n",
    "  model.add(feature_layer)\n",
    "\n",
    "  # Add one linear layer to the model to yield a simple linear regressor.\n",
    "  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n",
    "\n",
    "  # Construct the layers into a model that TensorFlow can execute.\n",
    "  model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "  return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the create_model and train_model functions.\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, feat, lbl, epochs, batch_size):\n",
    "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "  # Split the dataset into features and label.\n",
    "  features = {name:np.array(value) for name, value in train_data.items()} #transformando em dicionario\n",
    "  label = lbl.values\n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True)\n",
    "\n",
    "  # Get details that will be useful for plotting the loss curve.\n",
    "  epochs = history.epoch\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  rmse = hist[\"mean_squared_error\"]\n",
    "\n",
    "  return epochs, rmse   \n",
    "\n",
    "print(\"Defined the create_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.2771 - mean_squared_error: 2.2771\n",
      "Epoch 2/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.9477 - mean_squared_error: 1.9477\n",
      "Epoch 3/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.7407 - mean_squared_error: 1.7407\n",
      "Epoch 4/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.5716 - mean_squared_error: 1.5716\n",
      "Epoch 5/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.4303 - mean_squared_error: 1.4303\n",
      "Epoch 6/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.2975 - mean_squared_error: 1.2975\n",
      "Epoch 7/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.1784 - mean_squared_error: 1.1784\n",
      "Epoch 8/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.0692 - mean_squared_error: 1.0692\n",
      "Epoch 9/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9676 - mean_squared_error: 0.9676\n",
      "Epoch 10/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8761 - mean_squared_error: 0.8761\n",
      "Epoch 11/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7999 - mean_squared_error: 0.7999\n",
      "Epoch 12/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7245 - mean_squared_error: 0.7245\n",
      "Epoch 13/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6623 - mean_squared_error: 0.6623\n",
      "Epoch 14/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.6047 - mean_squared_error: 0.6047\n",
      "Epoch 15/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5538 - mean_squared_error: 0.5538\n",
      "Epoch 16/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5118 - mean_squared_error: 0.5118\n",
      "Epoch 17/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4783 - mean_squared_error: 0.4783\n",
      "Epoch 18/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4475 - mean_squared_error: 0.4475\n",
      "Epoch 19/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4207 - mean_squared_error: 0.4207\n",
      "Epoch 20/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4001 - mean_squared_error: 0.4001\n",
      "Epoch 21/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3803 - mean_squared_error: 0.3803\n",
      "Epoch 22/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3648 - mean_squared_error: 0.3648\n",
      "Epoch 23/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3516 - mean_squared_error: 0.3516\n",
      "Epoch 24/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3434 - mean_squared_error: 0.3434\n",
      "Epoch 25/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3313 - mean_squared_error: 0.3313\n",
      "Epoch 26/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3241 - mean_squared_error: 0.3241\n",
      "Epoch 27/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3184 - mean_squared_error: 0.3184\n",
      "Epoch 28/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3116 - mean_squared_error: 0.3116\n",
      "Epoch 29/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3079 - mean_squared_error: 0.3079\n",
      "Epoch 30/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3028 - mean_squared_error: 0.3028\n",
      "Epoch 31/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2966 - mean_squared_error: 0.2966\n",
      "Epoch 32/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2941 - mean_squared_error: 0.2941\n",
      "Epoch 33/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2896 - mean_squared_error: 0.2896\n",
      "Epoch 34/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2885 - mean_squared_error: 0.2885\n",
      "Epoch 35/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2880 - mean_squared_error: 0.2880\n",
      "Epoch 36/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2866 - mean_squared_error: 0.2866\n",
      "Epoch 37/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2823 - mean_squared_error: 0.2823\n",
      "Epoch 38/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2833 - mean_squared_error: 0.2833\n",
      "Epoch 39/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2797 - mean_squared_error: 0.2797\n",
      "Epoch 40/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2774 - mean_squared_error: 0.2774\n",
      "Epoch 41/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2766 - mean_squared_error: 0.2766\n",
      "Epoch 42/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2779 - mean_squared_error: 0.2779\n",
      "Epoch 43/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2757 - mean_squared_error: 0.2757\n",
      "Epoch 44/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2741 - mean_squared_error: 0.2741\n",
      "Epoch 45/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2778 - mean_squared_error: 0.2778\n",
      "Epoch 46/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2734 - mean_squared_error: 0.2734\n",
      "Epoch 47/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2725 - mean_squared_error: 0.2725\n",
      "Epoch 48/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2733 - mean_squared_error: 0.2733\n",
      "Epoch 49/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2716 - mean_squared_error: 0.2716\n",
      "Epoch 50/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2730 - mean_squared_error: 0.2730\n",
      "Epoch 51/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2725 - mean_squared_error: 0.2725\n",
      "Epoch 52/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2692 - mean_squared_error: 0.2692\n",
      "Epoch 53/150\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.2729 - mean_squared_error: 0.2729\n",
      "Epoch 54/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2729 - mean_squared_error: 0.2729\n",
      "Epoch 55/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2717 - mean_squared_error: 0.2717\n",
      "Epoch 56/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2744 - mean_squared_error: 0.2744\n",
      "Epoch 57/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2713 - mean_squared_error: 0.2713\n",
      "Epoch 58/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2709 - mean_squared_error: 0.2709\n",
      "Epoch 59/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2698 - mean_squared_error: 0.2698\n",
      "Epoch 60/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2696 - mean_squared_error: 0.2696\n",
      "Epoch 61/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2739 - mean_squared_error: 0.2739\n",
      "Epoch 62/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2694 - mean_squared_error: 0.2694\n",
      "Epoch 63/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2698 - mean_squared_error: 0.2698\n",
      "Epoch 64/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2713 - mean_squared_error: 0.2713\n",
      "Epoch 65/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2703 - mean_squared_error: 0.2703\n",
      "Epoch 66/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2721 - mean_squared_error: 0.2721\n",
      "Epoch 67/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2694 - mean_squared_error: 0.2694\n",
      "Epoch 68/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2705 - mean_squared_error: 0.2705\n",
      "Epoch 69/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2696 - mean_squared_error: 0.2696\n",
      "Epoch 70/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2719 - mean_squared_error: 0.2719\n",
      "Epoch 71/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2697 - mean_squared_error: 0.2697\n",
      "Epoch 72/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2689 - mean_squared_error: 0.2689\n",
      "Epoch 73/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2717 - mean_squared_error: 0.2717\n",
      "Epoch 74/150\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.2707 - mean_squared_error: 0.2707\n",
      "Epoch 75/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2697 - mean_squared_error: 0.2697\n",
      "Epoch 76/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2719 - mean_squared_error: 0.2719\n",
      "Epoch 77/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2687 - mean_squared_error: 0.2687\n",
      "Epoch 78/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2717 - mean_squared_error: 0.2717\n",
      "Epoch 79/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2702 - mean_squared_error: 0.2702\n",
      "Epoch 80/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2724 - mean_squared_error: 0.2724\n",
      "Epoch 81/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2698 - mean_squared_error: 0.2698\n",
      "Epoch 82/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2706 - mean_squared_error: 0.2706\n",
      "Epoch 83/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2709 - mean_squared_error: 0.2709\n",
      "Epoch 84/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2716 - mean_squared_error: 0.2716\n",
      "Epoch 85/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2707 - mean_squared_error: 0.2707\n",
      "Epoch 86/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2717 - mean_squared_error: 0.2717\n",
      "Epoch 87/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2712 - mean_squared_error: 0.2712\n",
      "Epoch 88/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2708 - mean_squared_error: 0.2708\n",
      "Epoch 89/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2709 - mean_squared_error: 0.2709\n",
      "Epoch 90/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2714 - mean_squared_error: 0.2714\n",
      "Epoch 91/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2709 - mean_squared_error: 0.2709\n",
      "Epoch 92/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2711 - mean_squared_error: 0.2711\n",
      "Epoch 93/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2730 - mean_squared_error: 0.2730\n",
      "Epoch 94/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2698 - mean_squared_error: 0.2698\n",
      "Epoch 95/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2689 - mean_squared_error: 0.2689\n",
      "Epoch 96/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2716 - mean_squared_error: 0.2716\n",
      "Epoch 97/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2713 - mean_squared_error: 0.2713\n",
      "Epoch 98/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2698 - mean_squared_error: 0.2698\n",
      "Epoch 99/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2700 - mean_squared_error: 0.2700\n",
      "Epoch 100/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2704 - mean_squared_error: 0.2704\n",
      "Epoch 101/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2691 - mean_squared_error: 0.2691\n",
      "Epoch 102/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2722 - mean_squared_error: 0.2722\n",
      "Epoch 103/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2698 - mean_squared_error: 0.2698\n",
      "Epoch 104/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2719 - mean_squared_error: 0.2719\n",
      "Epoch 105/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2709 - mean_squared_error: 0.2709\n",
      "Epoch 106/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2722 - mean_squared_error: 0.2722\n",
      "Epoch 107/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2708 - mean_squared_error: 0.2708\n",
      "Epoch 108/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2711 - mean_squared_error: 0.2711\n",
      "Epoch 109/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2701 - mean_squared_error: 0.2701\n",
      "Epoch 110/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2702 - mean_squared_error: 0.2702\n",
      "Epoch 111/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2699 - mean_squared_error: 0.2699\n",
      "Epoch 112/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2699 - mean_squared_error: 0.2699\n",
      "Epoch 113/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2709 - mean_squared_error: 0.2709\n",
      "Epoch 114/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2711 - mean_squared_error: 0.2711\n",
      "Epoch 115/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2704 - mean_squared_error: 0.2704\n",
      "Epoch 116/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2704 - mean_squared_error: 0.2704\n",
      "Epoch 117/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2709 - mean_squared_error: 0.2709\n",
      "Epoch 118/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2696 - mean_squared_error: 0.2696\n",
      "Epoch 119/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2719 - mean_squared_error: 0.2719\n",
      "Epoch 120/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2706 - mean_squared_error: 0.2706\n",
      "Epoch 121/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2710 - mean_squared_error: 0.2710\n",
      "Epoch 122/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2700 - mean_squared_error: 0.2700\n",
      "Epoch 123/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2689 - mean_squared_error: 0.2689\n",
      "Epoch 124/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2717 - mean_squared_error: 0.2717\n",
      "Epoch 125/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2698 - mean_squared_error: 0.2698\n",
      "Epoch 126/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2725 - mean_squared_error: 0.2725\n",
      "Epoch 127/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.2712 - mean_squared_error: 0.2712\n",
      "Epoch 128/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.2697 - mean_squared_error: 0.2697\n",
      "Epoch 129/150\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.2703 - mean_squared_error: 0.2703\n",
      "Epoch 130/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2716 - mean_squared_error: 0.2716\n",
      "Epoch 131/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2699 - mean_squared_error: 0.2699\n",
      "Epoch 132/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2694 - mean_squared_error: 0.2694\n",
      "Epoch 133/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2713 - mean_squared_error: 0.2713\n",
      "Epoch 134/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2723 - mean_squared_error: 0.2723\n",
      "Epoch 135/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2705 - mean_squared_error: 0.2705\n",
      "Epoch 136/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2701 - mean_squared_error: 0.2701\n",
      "Epoch 137/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2713 - mean_squared_error: 0.2713\n",
      "Epoch 138/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2691 - mean_squared_error: 0.2691\n",
      "Epoch 139/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2694 - mean_squared_error: 0.2694\n",
      "Epoch 140/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2715 - mean_squared_error: 0.2715\n",
      "Epoch 141/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2695 - mean_squared_error: 0.2695\n",
      "Epoch 142/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2725 - mean_squared_error: 0.2725\n",
      "Epoch 143/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2697 - mean_squared_error: 0.2697\n",
      "Epoch 144/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2716 - mean_squared_error: 0.2716\n",
      "Epoch 145/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2692 - mean_squared_error: 0.2692\n",
      "Epoch 146/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2700 - mean_squared_error: 0.2700\n",
      "Epoch 147/150\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2702 - mean_squared_error: 0.2702\n",
      "Epoch 148/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2703 - mean_squared_error: 0.2703\n",
      "Epoch 149/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2713 - mean_squared_error: 0.2713\n",
      "Epoch 150/150\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2695 - mean_squared_error: 0.2695\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluate the linear regression model against the test set:\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2574 - mean_squared_error: 0.2574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25743937492370605, 0.25743937492370605]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.005\n",
    "epochs = 150\n",
    "batch_size = 60\n",
    "label_name = \"median_house_value\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, my_feature_layer)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "epochs, mse = train_model(my_model, train_data, train_label, epochs, batch_size)\n",
    "plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "print(\"\\n Evaluate the linear regression model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_deep(my_learning_rate, my_feature_layer):\n",
    "\n",
    "    #modelo sequencial simples\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    #adicionar a  camada de feature columns\n",
    "    model.add(my_feature_layer)\n",
    "    \n",
    "    '''\n",
    "    model.add(tf.keras.layers.Dense(units=20, \n",
    "                                activation='relu',\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(l=0.01),\n",
    "                                name='Hidden1'))'''\n",
    "\n",
    "    # Define the first hidden layer with 20 nodes.   \n",
    "    model.add(tf.keras.layers.Dense(units=20, \n",
    "                                    activation='relu', \n",
    "                                    name='Hidden1'))\n",
    "    \n",
    "    # Define the second hidden layer with 12 nodes. \n",
    "    model.add(tf.keras.layers.Dense(units=12, \n",
    "                                    activation='relu', \n",
    "                                    name='Hidden2'))\n",
    "    \n",
    "    # Define the output layer.\n",
    "    model.add(tf.keras.layers.Dense(units=1,  \n",
    "                                    name='Output'))                              \n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
    "                  loss=\"mean_squared_error\",\n",
    "                  metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train_model é a mesma função de antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 2ms/step - loss: 0.8927 - mean_squared_error: 0.8927\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7858 - mean_squared_error: 0.7858\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7191 - mean_squared_error: 0.7191\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6576 - mean_squared_error: 0.6576\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6132 - mean_squared_error: 0.6132\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5712 - mean_squared_error: 0.5712\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5353 - mean_squared_error: 0.5353\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5000 - mean_squared_error: 0.5000\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4677 - mean_squared_error: 0.4677\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4379 - mean_squared_error: 0.4379\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4139 - mean_squared_error: 0.4139\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3883 - mean_squared_error: 0.3883\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3687 - mean_squared_error: 0.3687\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3489 - mean_squared_error: 0.3489\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3303 - mean_squared_error: 0.3303\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3145 - mean_squared_error: 0.3145\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2986 - mean_squared_error: 0.2986\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2844 - mean_squared_error: 0.2844\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2715 - mean_squared_error: 0.2715\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2597 - mean_squared_error: 0.2597\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2503 - mean_squared_error: 0.2503\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2423 - mean_squared_error: 0.2423\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2344 - mean_squared_error: 0.2344\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2276 - mean_squared_error: 0.2276\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2206 - mean_squared_error: 0.2206\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2142 - mean_squared_error: 0.2142\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2094 - mean_squared_error: 0.2094\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2042 - mean_squared_error: 0.2042\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1995 - mean_squared_error: 0.1995\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1953 - mean_squared_error: 0.1953\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1904 - mean_squared_error: 0.1904\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1870 - mean_squared_error: 0.1870\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1844 - mean_squared_error: 0.1844\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1802 - mean_squared_error: 0.1802\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1770 - mean_squared_error: 0.1770\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1740 - mean_squared_error: 0.1740\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1710 - mean_squared_error: 0.1710\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1698 - mean_squared_error: 0.1698\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1663 - mean_squared_error: 0.1663\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1639 - mean_squared_error: 0.1639\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1613 - mean_squared_error: 0.1613\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1599 - mean_squared_error: 0.1599\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1576 - mean_squared_error: 0.1576\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1555 - mean_squared_error: 0.1555\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1540 - mean_squared_error: 0.1540\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1524 - mean_squared_error: 0.1524\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1505 - mean_squared_error: 0.1505\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1488 - mean_squared_error: 0.1488\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1473 - mean_squared_error: 0.1473\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1460 - mean_squared_error: 0.1460\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1447 - mean_squared_error: 0.1447\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1429 - mean_squared_error: 0.1429\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1418 - mean_squared_error: 0.1418\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1405 - mean_squared_error: 0.1405\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1391 - mean_squared_error: 0.1391\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1383 - mean_squared_error: 0.1383\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1366 - mean_squared_error: 0.1366\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1357 - mean_squared_error: 0.1357\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1343 - mean_squared_error: 0.1343\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1330 - mean_squared_error: 0.1330\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1333 - mean_squared_error: 0.1333\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1306 - mean_squared_error: 0.1306\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1298 - mean_squared_error: 0.1298\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1292 - mean_squared_error: 0.1292\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1282 - mean_squared_error: 0.1282\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1267 - mean_squared_error: 0.1267\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1256 - mean_squared_error: 0.1256\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1248 - mean_squared_error: 0.1248\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1241 - mean_squared_error: 0.1241\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1230 - mean_squared_error: 0.1230\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1221 - mean_squared_error: 0.1221\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1209 - mean_squared_error: 0.1209\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1202 - mean_squared_error: 0.1202\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1193 - mean_squared_error: 0.1193\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1186 - mean_squared_error: 0.1186\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1176 - mean_squared_error: 0.1176\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1169 - mean_squared_error: 0.1169\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1161 - mean_squared_error: 0.1161\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.1151 - mean_squared_error: 0.1151\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1146 - mean_squared_error: 0.1146\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1130 - mean_squared_error: 0.1130\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1126 - mean_squared_error: 0.1126\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1116 - mean_squared_error: 0.1116\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1107 - mean_squared_error: 0.1107\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1102 - mean_squared_error: 0.1102\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1092 - mean_squared_error: 0.1092\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1085 - mean_squared_error: 0.1085\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1074 - mean_squared_error: 0.1074\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1066 - mean_squared_error: 0.1066\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1062 - mean_squared_error: 0.1062\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1054 - mean_squared_error: 0.1054\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1048 - mean_squared_error: 0.1048\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1038 - mean_squared_error: 0.1038\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1033 - mean_squared_error: 0.1033\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1024 - mean_squared_error: 0.1024\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1017 - mean_squared_error: 0.1017\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1008 - mean_squared_error: 0.1008\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1004 - mean_squared_error: 0.1004\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0998 - mean_squared_error: 0.0998\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0992 - mean_squared_error: 0.0992\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0988 - mean_squared_error: 0.0988\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.0980 - mean_squared_error: 0.0980\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.0968 - mean_squared_error: 0.0968\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0966 - mean_squared_error: 0.0966\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0957 - mean_squared_error: 0.0957\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0953 - mean_squared_error: 0.0953\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0944 - mean_squared_error: 0.0944\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0952 - mean_squared_error: 0.0952\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0927 - mean_squared_error: 0.0927\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0931 - mean_squared_error: 0.0931\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0926 - mean_squared_error: 0.0926\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0912 - mean_squared_error: 0.0912\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0901 - mean_squared_error: 0.0901\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0895 - mean_squared_error: 0.0895\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0890 - mean_squared_error: 0.0890\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0883 - mean_squared_error: 0.0883\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0878 - mean_squared_error: 0.0878\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0870 - mean_squared_error: 0.0870\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0868 - mean_squared_error: 0.0868\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0857 - mean_squared_error: 0.0857\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0850 - mean_squared_error: 0.0850\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0850 - mean_squared_error: 0.0850\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0840 - mean_squared_error: 0.0840\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0840 - mean_squared_error: 0.0840\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0828 - mean_squared_error: 0.0828\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0830 - mean_squared_error: 0.0830\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0820 - mean_squared_error: 0.0820\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0812 - mean_squared_error: 0.0812\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0810 - mean_squared_error: 0.0810\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0802 - mean_squared_error: 0.0802\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0798 - mean_squared_error: 0.0798\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0792 - mean_squared_error: 0.0792\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0788 - mean_squared_error: 0.0788\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0779 - mean_squared_error: 0.0779\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0781 - mean_squared_error: 0.0781\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0771 - mean_squared_error: 0.0771\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0766 - mean_squared_error: 0.0766\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0761 - mean_squared_error: 0.0761\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0763 - mean_squared_error: 0.0763\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0750 - mean_squared_error: 0.0750\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0748 - mean_squared_error: 0.0748\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0743 - mean_squared_error: 0.0743\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0741 - mean_squared_error: 0.0741\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0740 - mean_squared_error: 0.0740\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0726 - mean_squared_error: 0.0726\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0723 - mean_squared_error: 0.0723\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0717 - mean_squared_error: 0.0717\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0713 - mean_squared_error: 0.0713\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0710 - mean_squared_error: 0.0710\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0702 - mean_squared_error: 0.0702\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0696 - mean_squared_error: 0.0696\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0691 - mean_squared_error: 0.0691\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0690 - mean_squared_error: 0.0690\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0680 - mean_squared_error: 0.0680\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0682 - mean_squared_error: 0.0682\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0669 - mean_squared_error: 0.0669\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0669 - mean_squared_error: 0.0669\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0662 - mean_squared_error: 0.0662\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0660 - mean_squared_error: 0.0660\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0659 - mean_squared_error: 0.0659\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0650 - mean_squared_error: 0.0650\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0644 - mean_squared_error: 0.0644\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0643 - mean_squared_error: 0.0643\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0642 - mean_squared_error: 0.0642\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0638 - mean_squared_error: 0.0638\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0627 - mean_squared_error: 0.0627\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0623 - mean_squared_error: 0.0623\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0624 - mean_squared_error: 0.0624\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0620 - mean_squared_error: 0.0620\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0619 - mean_squared_error: 0.0619\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0613 - mean_squared_error: 0.0613\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0606 - mean_squared_error: 0.0606\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0602 - mean_squared_error: 0.0602\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0601 - mean_squared_error: 0.0601\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0594 - mean_squared_error: 0.0594\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0594 - mean_squared_error: 0.0594\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0589 - mean_squared_error: 0.0589\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0586 - mean_squared_error: 0.0586\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0585 - mean_squared_error: 0.0585\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0581 - mean_squared_error: 0.0581\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0575 - mean_squared_error: 0.0575\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0573 - mean_squared_error: 0.0573\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0569 - mean_squared_error: 0.0569\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0567 - mean_squared_error: 0.0567\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0565 - mean_squared_error: 0.0565\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0560 - mean_squared_error: 0.0560\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0562 - mean_squared_error: 0.0562\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0555 - mean_squared_error: 0.0555\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0553 - mean_squared_error: 0.0553\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0550 - mean_squared_error: 0.0550\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0552 - mean_squared_error: 0.0552\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0545 - mean_squared_error: 0.0545\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0542 - mean_squared_error: 0.0542\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0538 - mean_squared_error: 0.0538\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0536 - mean_squared_error: 0.0536\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0533 - mean_squared_error: 0.0533\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0534 - mean_squared_error: 0.0534\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0531 - mean_squared_error: 0.0531\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0526 - mean_squared_error: 0.0526\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0531 - mean_squared_error: 0.0531\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluate the linear regression model against the test set:\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1369 - mean_squared_error: 0.1369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13691923022270203, 0.13691923022270203]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.005\n",
    "epochs = 200\n",
    "batch_size = 60\n",
    "label_name = \"median_house_value\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model_deep(learning_rate, my_feature_layer)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "epochs, mse = train_model(my_model, train_data, train_label, epochs, batch_size)\n",
    "plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "print(\"\\n Evaluate the linear regression model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
